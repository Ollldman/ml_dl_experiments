{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da421240",
   "metadata": {},
   "source": [
    "## Что такое последовательные данные\n",
    "\n",
    "**Последовательные данные** — это данные, где каждому объекту сопоставляется упорядоченный набор наблюдений: слов, сигналов с датчиков, цен на акции и т. п. У разных объектов может быть разная длина последовательности, например, у одного товара длинное текстовое описание, у другого — короткое. Важность порядка и непостоянная длина здесь главные вещи, которые нужно знать.\n",
    "\n",
    "Четыре небольших изображения:\n",
    "На первом график температуры по дням, изображающий временной ряд. \n",
    "\n",
    "На втором — текст «Коллекционный световой меч Энакина Скайуокера», изображающий последовательный текст. \n",
    "\n",
    "На третьем — волна звука, показывающая последовательный ряд.\n",
    "\n",
    "На четвёртом — линия из нескольких последовательных геолокаций.\n",
    "\n",
    "Примеры последовательных данных:\n",
    "\n",
    "- Последовательность замеров среднедневной температуры воздуха.\n",
    "- Текст — последовательность слов, в которой важен их порядок.\n",
    "- Аудиодорожка — последовательность звуковых колебаний во времени.\n",
    "- История передвижений объекта по карте — последовательность географических координат.\n",
    "\n",
    "## Где применяются последовательные данные\n",
    "\n",
    "На последовательных данных можно решать все классические типы задач машинного обучения, решаемые на табличных данных:\n",
    "\n",
    "- Классификация: по набору признаков объекта определить класс этого объекта. Классы при этом заранее зафиксированы в постановке задачи.\n",
    "- Регрессия: по набору признаков объекта предсказать численную характеристику этого объекта. Таргетом здесь является не класс, а вещественное число.\n",
    "- Кластеризация: сгруппировать объекты по группам. Количество групп при этом может быть заранее задано в постановке задачи.\n",
    "\n",
    "Определить тип задачи можно просто по типу предсказываемой величины.\n",
    "\n",
    "Специфичные для разных областей типы задач точно так же можно решать на последовательных данных:\n",
    "\n",
    "- Рекомендации: по набору признаков о пользователе порекомендовать ему наиболее подходящий айтем.\n",
    "- Матчинг: для двух последовательностей понять, описывают ли они один и тот же объект.\n",
    "- Ранжирование: отсортировать последовательности по их релевантности.\n",
    "\n",
    "И другое.\n",
    "\n",
    "Как и в классическом машинном обучении, тип задачи здесь определяется по области, в которой она решается, — и специфические типы задач имеют собственные специфические для области названия.\n",
    "\n",
    "Но бывает, что требуется работа именно с последовательными, а не с табличными данными — и под это есть отдельные типы задач:\n",
    "\n",
    "- Классификация элементов последовательности: для каждого элемента последовательности предсказать класс.\n",
    "- Генерация: сгенерировать новую последовательность данных на основе уже имеющейся. Новая последовательность может быть длины 1 — тогда задача сводится к предсказанию следующего элемента последовательности. А может быть очень большой длины в тысячи и десятки тысяч элементов.\n",
    "\n",
    "### Обработка текста (NLP)\n",
    "\n",
    "- Классификация: определение темы статьи, тональности отзыва.\n",
    "- Регрессия: оценить долю плагиата в тексте.\n",
    "- Распознавание сущностей: извлечь из текстового документа имя и фамилию автора.\n",
    "- Генерация: сгенерировать автоматический ответ в чат-боте поддержки.\n",
    "\n",
    "### Обработка видео\n",
    "Видео — это последовательность картинок, а значит, с ним можно работать как с последовательными данными.\n",
    "\n",
    "- Детекция: автопилот должен «видеть» другие автомобили на дороге, пешеходов, дорожные знаки.\n",
    "- Кластеризация: компаниям, работающим с видео, полезно автоматически группировать их по разным категориям.\n",
    "\n",
    "### Анализ временных рядов\n",
    "\n",
    "- Прогнозирование: предсказание курса валют, температуры, спроса на продукт.\n",
    "- Обнаружение аномалий: выявление сбоев в работе оборудования по сенсорам.\n",
    "- Классификация: классификация ЭКГ-сигналов, типов движения пользователя по акселерометру.\n",
    "\n",
    "### Анализ поведения пользователей\n",
    "\n",
    "- Рекомендательные системы: по истории просмотров порекомендовать новый фильм.\n",
    "- Предсказание следующего действия: например, кликнет ли пользователь на рекламу и как повысить вероятность его клика.\n",
    "\n",
    "### Множество других задач\n",
    "\n",
    "На самом деле задач с последовательными данными гораздо больше и перечислить все невозможно."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205fb94a",
   "metadata": {},
   "source": [
    "## Небольшая практика\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac0d4d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet\n",
      "0   @user when a father is dysfunctional and is s...\n",
      "1  @user @user thanks for #lyft credit i can't us...\n",
      "2                                bihday your majesty\n",
      "3  #model   i love u take with u all the time in ...\n",
      "4             factsguide: society now    #motivation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from ml_dl_experiments.\\\n",
    "    dl.dl_modules.\\\n",
    "    RNN_transformers.\\\n",
    "    plot_time_series import plot_time_series\n",
    "\n",
    "url: str = \"https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\"\n",
    "\n",
    "texts_df: pd.DataFrame = pd.read_csv(url)\n",
    "texts_df = texts_df[['tweet']]\n",
    "\n",
    "print(texts_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44ae862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "words",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "369f6ade-6d64-4ebe-81c4-9aba2d34cc59",
       "rows": [
        [
         "0",
         "['@user', 'when', 'a', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'he', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction.', '#run']"
        ],
        [
         "1",
         "['@user', '@user', 'thanks', 'for', '#lyft', 'credit', 'i', \"can't\", 'use', 'cause', 'they', \"don't\", 'offer', 'wheelchair', 'vans', 'in', 'pdx.', '#disapointed', '#getthanked']"
        ],
        [
         "2",
         "['bihday', 'your', 'majesty']"
        ],
        [
         "3",
         "['#model', 'i', 'love', 'u', 'take', 'with', 'u', 'all', 'the', 'time', 'in', 'urð\\x9f\\x93±!!!', 'ð\\x9f\\x98\\x99ð\\x9f\\x98\\x8eð\\x9f\\x91\\x84ð\\x9f\\x91', 'ð\\x9f\\x92¦ð\\x9f\\x92¦ð\\x9f\\x92¦']"
        ],
        [
         "4",
         "['factsguide:', 'society', 'now', '#motivation']"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "0    [@user, when, a, father, is, dysfunctional, an...\n",
       "1    [@user, @user, thanks, for, #lyft, credit, i, ...\n",
       "2                              [bihday, your, majesty]\n",
       "3    [#model, i, love, u, take, with, u, all, the, ...\n",
       "4             [factsguide:, society, now, #motivation]\n",
       "Name: words, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "\n",
    "# удаление повторяющихся пробелов и приведение к нижнему регистру\n",
    "texts_df['tweet'] = texts_df['tweet'].apply(lambda x: re.sub(r'\\s+', ' ', x.strip().lower()))\n",
    "\n",
    "\n",
    "# получение списка слов по строке \n",
    "texts_df['words'] = texts_df['tweet'].apply(lambda x: x.split())\n",
    "\n",
    "texts_df['words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076dca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user: 17291\n",
      "the: 10065\n",
      "to: 9768\n",
      "a: 6261\n",
      "i: 5655\n",
      "you: 4949\n",
      "and: 4831\n",
      "in: 4570\n",
      "for: 4435\n",
      "of: 4152\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Объединяем все списки слов в один длинный список\n",
    "all_words = [word for tweet in texts_df['words'] for word in tweet]\n",
    "\n",
    "\n",
    "# Подсчитываем частоты\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "\n",
    "# Получаем топ-10\n",
    "\n",
    "top_10 = word_counts.most_common(10)\n",
    "\n",
    "\n",
    "# Выводим\n",
    "for word, count in top_10:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176aeca",
   "metadata": {},
   "source": [
    "# Численное представление последовательных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a08f46",
   "metadata": {},
   "source": [
    "## Примеры численных представлений последовательных данных\n",
    "\n",
    "Изначальный вид последовательных данных может быть самый разный — от аудиосигнала до последовательных измерений метеодатчиков. Поэтому и методы преобразования таких данных в численный вид тоже разные. \n",
    "\n",
    "### Биоинформатика и Label Encoding\n",
    "\n",
    "В этом курсе мы не рассматриваем биоинформатику и геномные последовательности, но этот пример важен для понимания методологии, используемой повсеместно. \n",
    "\n",
    "В современной биоинформатике часто применяют машинное обучение, но как подать геномную последовательность на вход нейросети? Геномная последовательность представляет собой последовательность букв A, G, C, T: ACGTATG.... В таком случае можно применить методику Label Encoding — каждой букве сопоставить уникальное число:\n",
    "```\n",
    "A -> 0\n",
    "\n",
    "G -> 1\n",
    "\n",
    "C -> 2\n",
    "\n",
    "T -> 3 \n",
    "```\n",
    "Тогда последовательность ACGTATG в численном виде будет представлена как 0213031.\n",
    "\n",
    "### Финансовые данные и агрегация временных рядов\n",
    "\n",
    "Работа с финансовыми данными, особенно предсказание цены актива, — частая задача в машинном обучении. Но финансовые данные могут поступать на вход в сложном виде: например, для цен на акции за временной промежуток может фиксироваться цена открытия, цена закрытия, объём торгов и многое другое. Чтобы получить из этих данных один временной ряд, который можно предсказывать нейросетью, используется агрегация временных рядов.\n",
    "\n",
    "Выбор метода агрегации зависит от решаемой задачи:\n",
    "\n",
    "- Можно выбрать из временного ряда лишь один показатель, который хочется прогнозировать (например, цену закрытия).\n",
    "- Можно агрегировать разные показатели в один — например, взять среднюю/медианную котировку в течение дня или посчитать среднее между ценой закрытия и ценой открытия.\n",
    "\n",
    "### Аудиосигналы и спектрограммы\n",
    "\n",
    "Аудиосигналы — это последовательные данные, представляющие изменение звуковых колебаний во времени. При работе с аудио нейросети принимают на вход спектрограмму — двумерное изображение, иллюстрирующее этот сигнал.\n",
    "\n",
    "Получается, что численное представление последовательных данных не всегда является одномерной последовательностью чисел. Оно может быть представлено и изображением, то есть двумерной матрицей. И нейросеть будет работать с этой матрицей как с картинкой."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a4703",
   "metadata": {},
   "source": [
    "### Токенизация текстов\n",
    "\n",
    "Сперва текст разбивается на отдельные единицы — токены, а затем токены рассматриваются как категориальные данные, то есть каждый токен имеет соответствующее ему число. Токен может представлять собой или последовательность символов, в том числе слова. Преобразование текста в последовательность токенов называется токенизацией. У неё есть несколько типов.\n",
    "\n",
    "**Пословная токенизация (Word-Level Tokenization)**\n",
    "\n",
    "Каждый токен представляет собой отдельное слово. Задаём соответствие между токенами и числами — получаем последовательность чисел:\n",
    "\n",
    "```\n",
    "\"i love deep learning\" -> [\"i\", \"love\", \"deep\", \"learning\"] -> [256, 140, 320, 521]\n",
    "\n",
    "\"i love deep diving\" -> [\"i\", \"love\", \"deep\", \"diving\"] -> [256, 140, 320, 1280] \n",
    "```\n",
    "Такая токенизация требует большого размера словаря из всех слов и словоформ. А ещё могут возникнуть проблемы с токенизацией слова, которого нет в словаре, — такие проблемы называются Out-of-Vocabulary. Зато последовательности чисел в итоге получаются короткими, нейросетям будет проще с такими работать.\n",
    "\n",
    "**Посимвольная токенизация (Char-Level Tokenization)**\n",
    "Здесь строка рассматривается как последовательность символов и каждый символ имеет свой номер — например, как в кодировке ASCII:\n",
    "```\n",
    "\"text\" -> ['t', 'e', 'x', 't'] -> [116, 101, 120, 116]\n",
    "\"tokens\" -> ['t', 'o', 'k', 'e', 'n', 's'] -> [116, 111, 107, 101, 110, 115] \n",
    "```\n",
    "Удобство здесь в том, что размер словаря очень маленький, — символов всего не очень много. Однако последовательности получаются довольно длинными и на них труднее будет учиться.\n",
    "\n",
    "**Подсловная токенизация (Subword Tokenization)**\n",
    "\n",
    "Здесь текст разбивается на части меньшие, чем слова, но большие, чем отдельные символы. Таким образом соблюдается баланс между длиной получившейся последовательности и размером словаря. Есть много разных методов подсловной токенизации. Один из наиболее популярных методов — Byte-Pair Encoding (BPE).\n",
    "\n",
    "Как работает BPE:\n",
    "\n",
    "- Начинаем с посимвольной токенизации всех слов в корпусе.\n",
    "- Подсчитываем частотность всех пар символов.\n",
    "- На каждом шаге объединяем самую частую пару в один токен.\n",
    "- Повторяем, пока не достигнем заданного размера словаря.\n",
    "\n",
    "Пример подсловной токенизации:\n",
    "```\n",
    "\"unhappiness\" -> [\"un\", \"happi\", \"ness\"] -> [470, 819, 344] \n",
    "```\n",
    "В современных языковых моделях используется именно подсловная токенизация. Разные модели могут использовать разные методы подсловной токенизации и наборы токенов. То, что работает для одной модели, может не работать для другой.\n",
    "\n",
    "Посмотреть, как токенизируются тексты для разных моделей, можно с помощью библиотеки `transformers` от HuggingFace. В этой библиотеке хранится много предобученных моделей и токенизаторов для решения разных задач NLP. \n",
    "\n",
    "```py\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(tokenizer.tokenize('ai can be misinterpreted as artificial intelligence'))\n",
    "```\n",
    "\n",
    "В нём используется подсловный токенизатор для популярной модели Bert от Google. Класс `AutoTokenizer` и его метод `from_pretrained` принимает на вход название модели и создаёт токенизатор. После создания токенизатора `tokenizer` можно воспользоваться его методом `.tokenize()`, передав в качестве аргумента строку. На выходе получится список из токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8b77b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cef9514d9c4cf8848b5f94dcdd847a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527a9d1f98c44e6a944ca0bc9b463710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b773fca2f3541858fa1c48eaf6630c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4f562c1034436aaa0061a9f235682a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'can', 'be', 'mis', '##int', '##er', '##pre', '##ted', 'as', 'artificial', 'intelligence']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(tokenizer.tokenize('ai can be misinterpreted as artificial intelligence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8f1ce9",
   "metadata": {},
   "source": [
    "Перейдем к практике предобработки текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd4c34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       " 'Ok lar... Joking wif u oni...',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'U dun say so early hor... U c already then say...',\n",
       " \"Nah I don't think he goes to usf, he lives around here though\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv\"\n",
    "response = requests.get(url)\n",
    "corpus = [\n",
    "    line.split('\\t')[1] \n",
    "    for line in response.text.strip().split('\\n')\n",
    "]\n",
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9288211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пословная токенизация:\n",
      "['Go', 'until', 'jurong', 'point,', 'crazy..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'Cine', 'there', 'got', 'amore', 'wat...']\n",
      "['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...']\n",
      "['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', \"rate)T&C's\", 'apply', \"08452810075over18's\"]\n",
      "Посимвольная токенизация:\n",
      "['G', 'o', ' ', 'u', 'n', 't', 'i', 'l', ' ', 'j', 'u', 'r', 'o', 'n', 'g', ' ', 'p', 'o', 'i', 'n', 't', ',', ' ', 'c', 'r', 'a', 'z', 'y', '.', '.', ' ', 'A', 'v', 'a', 'i', 'l', 'a', 'b', 'l', 'e', ' ', 'o', 'n', 'l', 'y', ' ', 'i', 'n', ' ', 'b', 'u', 'g', 'i', 's', ' ', 'n', ' ', 'g', 'r', 'e', 'a', 't', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 'l', 'a', ' ', 'e', ' ', 'b', 'u', 'f', 'f', 'e', 't', '.', '.', '.', ' ', 'C', 'i', 'n', 'e', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'g', 'o', 't', ' ', 'a', 'm', 'o', 'r', 'e', ' ', 'w', 'a', 't', '.', '.', '.']\n",
      "['O', 'k', ' ', 'l', 'a', 'r', '.', '.', '.', ' ', 'J', 'o', 'k', 'i', 'n', 'g', ' ', 'w', 'i', 'f', ' ', 'u', ' ', 'o', 'n', 'i', '.', '.', '.']\n",
      "['F', 'r', 'e', 'e', ' ', 'e', 'n', 't', 'r', 'y', ' ', 'i', 'n', ' ', '2', ' ', 'a', ' ', 'w', 'k', 'l', 'y', ' ', 'c', 'o', 'm', 'p', ' ', 't', 'o', ' ', 'w', 'i', 'n', ' ', 'F', 'A', ' ', 'C', 'u', 'p', ' ', 'f', 'i', 'n', 'a', 'l', ' ', 't', 'k', 't', 's', ' ', '2', '1', 's', 't', ' ', 'M', 'a', 'y', ' ', '2', '0', '0', '5', '.', ' ', 'T', 'e', 'x', 't', ' ', 'F', 'A', ' ', 't', 'o', ' ', '8', '7', '1', '2', '1', ' ', 't', 'o', ' ', 'r', 'e', 'c', 'e', 'i', 'v', 'e', ' ', 'e', 'n', 't', 'r', 'y', ' ', 'q', 'u', 'e', 's', 't', 'i', 'o', 'n', '(', 's', 't', 'd', ' ', 't', 'x', 't', ' ', 'r', 'a', 't', 'e', ')', 'T', '&', 'C', \"'\", 's', ' ', 'a', 'p', 'p', 'l', 'y', ' ', '0', '8', '4', '5', '2', '8', '1', '0', '0', '7', '5', 'o', 'v', 'e', 'r', '1', '8', \"'\", 's']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = [\n",
    "    message.split()\n",
    "    for message in corpus\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Пословная токенизация:\")\n",
    "for tokens in word_tokens[:3]:\n",
    "    print(tokens)\n",
    "\n",
    "\n",
    "char_tokens = [\n",
    "    list(message)\n",
    "    for message in corpus\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Посимвольная токенизация:\")\n",
    "for tokens in char_tokens[:3]:\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184d7ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подсловная токенизация (BPE):\n",
      "[['go', 'until', 'ju', '##rong', 'point', ',', 'crazy', '.', '.', 'available', 'only', 'in', 'bug', '##is', 'n', 'great', 'world', 'la', 'e', 'buffet', '.', '.', '.', 'ci', '##ne', 'there', 'got', 'amore', 'wat', '.', '.', '.'], ['ok', 'la', '##r', '.', '.', '.', 'joking', 'wi', '##f', 'u', 'on', '##i', '.', '.', '.'], ['free', 'entry', 'in', '2', 'a', 'w', '##k', '##ly', 'com', '##p', 'to', 'win', 'fa', 'cup', 'final', 't', '##kt', '##s', '21st', 'may', '2005', '.', 'text', 'fa', 'to', '87', '##12', '##1', 'to', 'receive', 'entry', 'question', '(', 'st', '##d', 'tx', '##t', 'rate', ')', 't', '&', 'c', \"'\", 's', 'apply', '08', '##45', '##28', '##100', '##75', '##over', '##18', \"'\", 's']]\n"
     ]
    }
   ],
   "source": [
    "# импортируйте класс AutoTokenizer \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# создайте объект tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "bpe_tokens = []\n",
    "\n",
    "\n",
    "print(\"Подсловная токенизация (BPE):\")\n",
    "for message in corpus:\n",
    "    tokens = tokenizer.tokenize(message)\n",
    "    bpe_tokens.append(tokens)\n",
    "\n",
    "\n",
    "print(bpe_tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7ca4e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сравнение средних длин:\n",
      "Пословная: 15.59167563688554\n",
      "Посимвольная: 80.47829207032652\n",
      "Подсловная: 23.401865805525656\n",
      "Топ-10 токенов (пословная): [('to', 2145), ('you', 1626), ('I', 1469), ('a', 1337), ('the', 1207), ('and', 858), ('in', 800), ('is', 788), ('i', 748), ('u', 698)]\n",
      "Топ-10 токенов (посимвольная): [(' ', 81961), ('e', 33204), ('o', 27278), ('t', 25768), ('a', 23478), ('n', 20227), ('i', 19077), ('s', 17023), ('r', 16677), ('l', 14728)]\n",
      "Топ-10 токенов (подсловная): [('.', 11214), ('i', 3029), ('to', 2298), ('you', 2249), (',', 1980), (\"'\", 1879), ('?', 1550), ('a', 1454), ('u', 1407), ('!', 1397)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_avg_message_len(tokenized_messages):\n",
    "    # напишите код расчёта средней длины токенизированного сообщения\n",
    "    total_summs: list = []\n",
    "    for message in tokenized_messages:\n",
    "        l = len(message)\n",
    "        total_summs.append(l)\n",
    "\n",
    "    return np.mean(total_summs)\n",
    "\n",
    "\n",
    "print(\"\\nСравнение средних длин:\")\n",
    "print(\"Пословная:\", get_avg_message_len(word_tokens))\n",
    "print(\"Посимвольная:\", get_avg_message_len(char_tokens))\n",
    "print(\"Подсловная:\", get_avg_message_len(bpe_tokens))\n",
    "\n",
    "\n",
    "# Сглаживаем списки (список списков → плоский список)\n",
    "flat_word_tokens = [token for message in word_tokens for token in message]\n",
    "flat_char_tokens = [token for message in char_tokens for token in message]\n",
    "flat_bpe_tokens = [token for message in bpe_tokens for token in message]\n",
    "\n",
    "\n",
    "# Найдите топ-10 токенов для каждого метода\n",
    "word_freq_top10 = Counter(flat_word_tokens).most_common(10)\n",
    "char_freq_top10 = Counter(flat_char_tokens).most_common(10)\n",
    "bpe_freq_top10 = Counter(flat_bpe_tokens).most_common(10)\n",
    "\n",
    "\n",
    "# Вывод топ-10\n",
    "\n",
    "print(\"Топ-10 токенов (пословная):\", word_freq_top10)\n",
    "print(\"Топ-10 токенов (посимвольная):\", char_freq_top10)\n",
    "print(\"Топ-10 токенов (подсловная):\", bpe_freq_top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0a59a",
   "metadata": {},
   "source": [
    "Ожидаемо, что самый популярный символ — пробел. Также видно, что самые популярные токены в подсловной и пословной токенизации — это наиболее часто используемые слова. Это логично, ведь если слово встречается часто, то его части объединяются в один токен при алгоритме подсловной токенизации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4fdafb",
   "metadata": {},
   "source": [
    "# Подготовка данных для обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3552dbf",
   "metadata": {},
   "source": [
    "## Проблема разных длин последовательностей\n",
    "\n",
    "Одна из особенностей последовательных данных заключается в том, что последовательности в датасете могут быть разной длины. Например, для набора текстов какие-то из них могут быть длинными, а какие-то — короткими.\n",
    "\n",
    "Нейронная сеть представляет собой последовательность матричных операций и нелинейных функций между ними. Поэтому входные данные должны быть представлены в виде матрицы — двумерного массива размера (sequence_num, sequence_len), где:\n",
    "\n",
    "- sequence_num — количество последовательностей.\n",
    "- sequence_len — длина последовательностей.\n",
    "\n",
    "Но как упаковать набор последовательностей разной длины в матрицу? Для этого всех их придётся привести к одной длине. Сделать это поможет метод padding — самый популярный метод приведения всех последовательностей к одной длине.\n",
    "\n",
    "### Суть метода padding\n",
    "\n",
    "Сперва нужно задать длину max_len, к которой нужно привести все последовательности. Обычно её выбор зависит от специфики датасета и постановки задачи.\n",
    "Затем для каждой последовательности нужно сделать следующее:\n",
    "\n",
    "- Если длина последовательности больше max_len, то нужно отрезать лишние токены.\n",
    "- Если длина меньше max_len, то нужно дополнить последовательность специальными токенами до нужной длины.\n",
    "\n",
    "Пример:\n",
    "```py\n",
    "# предложения до padding'а\n",
    "[\n",
    "    ['i', 'love', 'coding', 'and', 'ml'],\n",
    "    ['i', 'love', 'coding'],\n",
    "    ['i', 'love', 'coding', 'math', 'and', 'ml']\n",
    "]\n",
    "\n",
    "# после padding'а до длины 5 токенов\n",
    "[\n",
    "    ['i', 'love', 'coding', 'and', 'ml'],\n",
    "    ['i', 'love', 'coding', '<PAD_TOKEN>', '<PAD_TOKEN>'],\n",
    "    ['i', 'love', 'coding', 'math', 'and']\n",
    "] \n",
    "```\n",
    "В этом примере первое предложение не изменилось, так как оно изначально желаемой длины. Второе прeдложение дополнилось специальными токенами. Третье — стало короче за счёт удаления всех токенов после превышения желаемой длины max_len.\n",
    "\n",
    "Действительно, после padding'а модель должна понимать, какие элементы последовательности настоящие, а какие — просто padding. Это позволит оптимизировать обучение и лишний раз не считать градиенты и функции потерь по бесполезным токенам. Для этого используется masking — бинарная маска. Она показывает модели, где находится полезная информация (1), а где padding (0).\n",
    "Пример маски для предыдущих последовательностей:\n",
    "```py\n",
    "# предложения после padding'а до длины 5 токенов\n",
    "[\n",
    "    ['i', 'love', 'coding', 'and', 'ml'],\n",
    "    ['i', 'love', 'coding', '<PAD_TOKEN>', '<PAD_TOKEN>'],\n",
    "    ['i', 'love', 'coding', 'math', 'and']\n",
    "]\n",
    "\n",
    "\n",
    "# маска\n",
    "[\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 0, 0],\n",
    "    [1, 1, 1, 1, 1]\n",
    "] \n",
    "```\n",
    "Теперь нейросеть может проигнорировать padding во время обучения и вычисления потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4555df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': tensor([[ 5,  9, 12,  0,  0],\n",
      "        [ 2, 45, 23, 11,  0]]), 'masks': tensor([[1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0]]), 'labels': tensor([1, 0])}\n",
      "{'texts': tensor([[12,  0,  0,  0,  0]]), 'masks': tensor([[1, 0, 0, 0, 0]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "# импортируем нужные библиотеки\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# для совместимости с другими методами класс нашего датасета наследуем от класса Dataset из PyTorch\n",
    "class PaddedDataset(Dataset):\n",
    "    # в конструкторе просто сохраняем все данные \n",
    "    def __init__(self, texts, labels, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # метод __len__ возвращает количество объектов в датасете\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # метод __getitem__ возвращает элемент датасета с индексом idx\n",
    "    def __getitem__(self, idx):\n",
    "        # получаем текст и его класс по индексу\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # вручную реализуем padding и masking\n",
    "        padded = text + [0] * (self.max_len - len(text))\n",
    "        mask = [1] * len(text) + [0] * (self.max_len - len(text))\n",
    "\n",
    "        # возвращаем текст после padding'а, маску и класс\n",
    "        return {\n",
    "            'texts': torch.tensor(padded, dtype=torch.long),\n",
    "            'masks': torch.tensor(mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "texts = [[5, 9, 12], [2, 45, 23, 11], [12]]\n",
    "labels = [1, 0, 1]\n",
    "max_len = 5\n",
    "\n",
    "# создаем объект датасета\n",
    "dataset = PaddedDataset(texts, labels, max_len)\n",
    "\n",
    "# используем готовый DataLoader из PyTorch\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# выводим батчи, который формируются в даталоадере\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f11c2e",
   "metadata": {},
   "source": [
    "## На уровне функции collate_fn в torch.DataLoader\n",
    "\n",
    "Здесь padding добавляется на лету только для текущего батча. Этот подход более гибкий: он позволяет экономить память и вычисления.\n",
    "\n",
    "Представьте, что в батч попали только короткие последовательности. Тогда нет смысла дополнять их всех до длинных — можно просто сделать padding до самой длинной последовательности в батче и сэкономить ресурсы. Это главный плюс этого подхода в сравнении с первым.\n",
    "\n",
    "Как это выглядит в коде: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f33c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'texts': tensor([[ 5,  9, 12,  0],\n",
      "        [ 2, 45, 23, 11]]), 'masks': tensor([[1, 1, 1, 0],\n",
      "        [1, 1, 1, 1]]), 'labels': tensor([1, 0])}\n",
      "{'texts': tensor([[12]]), 'masks': tensor([[1]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "# имортируем нужные библиотеки\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# создаем датасет, наследуясь от класса Dataset из PyTorch\n",
    "class RawDataset(Dataset):\n",
    "    # в конструкторе просто сохраняем тексты и классы\n",
    "    def __init__(self, texts, labels, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # возвращаем размер датасета (кол-во текстов)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # возвращаем текст и его класс\n",
    "        # для текста ограничиваем длину\n",
    "        # не делаем никаких доп. преобразований как padding и masking\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx][:self.max_len], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # список текстов и классов из батча\n",
    "    texts = [item['text'] for item in batch]\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "\n",
    "    # дополняем тексты в батче padding'ом\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # считаем маски\n",
    "    masks = (padded_texts != 0).long()\n",
    "\n",
    "    # возвращаем преобразованный батч\n",
    "    return {\n",
    "        'texts': padded_texts,\n",
    "        'masks': masks,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "texts = [[5, 9, 12], [2, 45, 23, 11], [12]]\n",
    "labels = [1, 0, 1]\n",
    "max_len = 5\n",
    "\n",
    "# создаем объект датасета\n",
    "dataset = RawDataset(texts, labels, max_len)\n",
    "\n",
    "# пользуемся готовым даталоадером из PyTorch, но с кастомной функцией collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0744d16",
   "metadata": {},
   "source": [
    "В первом батче пэддинг произошёл до длины 4, так как это размер самого длинного текста в батче. Во втором батче пэддинга и нет вовсе, так как батч состоит из одного элемента и его нет смысла удлинять. Таким образом, подход с кастомной функцией `collate_fn` помогает экономить память, а при обучении нейросетей ещё и поможет сэкономить вычислительные мощности.\n",
    "\n",
    "В примере выше `torch.Dataset` не делает никаких преобразований с текстами. Он просто их возвращает. Единственное, на что стоит обратить внимание, — если длина текста слишком большая, он ограничивает её `max_len` токенами.\n",
    "\n",
    "Магия начинается при реализации функции `collate_fn`:\n",
    "\n",
    "1. Функция принимает на вход список элементов датасета, из них нужно сформировать батч.\n",
    "2. С помощью функции `pad_sequence` дополняет их до длины, равной длине максимальной последовательности в батче. В качестве `padding_token` используется 0.\n",
    "3. При сравнении токенов текстов после пэддинга с нулём (`padding_token`) считается маска.\n",
    "4. Возвращаемые значения такие же, как и в первом примере, — тексты, маски, лейблы.\n",
    "\n",
    "Таким образом, передавая функцию `collate_fn` в конструктор DataLoader'а, мы, по сути, передаём инструкцию, как формировать батчи из элементов датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bccc023",
   "metadata": {},
   "source": [
    "### Когда и что использовать:\n",
    "\n",
    "|На уровне `torch.Dataset`|На уровне функции `collate_fn` в `torch.DataLoader`|\n",
    "|-----------|------------|\n",
    "|Все последовательности в финальных данных должны быть фиксированной длины|Хочется более оптимально и гибко собирать батчи|\n",
    "|Нет цели экономить память и вычисления|Важно оптимизировать процесс обучения|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe5e8b",
   "metadata": {},
   "source": [
    "К практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a7edd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts:\n",
      " tensor([[12, 56, 78, 90, 43, 22, 11,  8],\n",
      "        [ 5,  9, 12, 34, 23, 76, 89,  0],\n",
      "        [ 2, 45, 23,  0,  0,  0,  0,  0]])\n",
      "Masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "Labels: tensor([0, 0, 1])\n",
      "Lengths: tensor([8, 7, 3])\n",
      "========================================\n",
      "Texts:\n",
      " tensor([[ 3,  7,  8,  5,  1,  4,  6, 10, 11],\n",
      "        [65, 12, 99, 54,  0,  0,  0,  0,  0]])\n",
      "Masks:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "Labels: tensor([1, 2])\n",
      "Lengths: tensor([9, 4])\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "texts = [\n",
    "    [5, 9, 12, 34, 23, 76, 89],          # len = 7\n",
    "    [2, 45, 23],                         # len = 3\n",
    "    [12, 56, 78, 90, 43, 22, 11, 8],     # len = 8\n",
    "    [65, 12, 99, 54],                    # len = 4\n",
    "    [3, 7, 8, 5, 1, 4, 6, 10, 11],       # len = 9\n",
    "]\n",
    "\n",
    "\n",
    "labels = [0, 1, 0, 2, 1]\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        # сложите тексты и классы в переменные датасета\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # посчитайте размер датасета\n",
    "        return len(self.texts)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # верните элемент датасета\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\n",
    "            \"text\": torch.tensor(text, dtype=torch.long),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    sorted_batch = sorted(\n",
    "        [(item[\"text\"], item[\"label\"]) for item in batch],\n",
    "        key=lambda x: len(x[0]),\n",
    "        reverse=True)\n",
    "    texts = [i[0] for i in sorted_batch]\n",
    "    labels = torch.stack([i[1] for i in sorted_batch])\n",
    "    lengths = torch.stack([torch.tensor(len(i[0]), dtype=torch.long) for i in sorted_batch])\n",
    "    # дополните тексты пэддингом\n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "\n",
    "    # посчитайте маску для батча\n",
    "    masks = (padded_texts != 0).long()\n",
    "    return {\n",
    "        'texts': padded_texts,     # (batch_size, max_len)\n",
    "        'masks': masks,            # (batch_size, max_len)\n",
    "        'labels': labels,          # (batch_size,)\n",
    "        \"lengths\": lengths\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = TextDataset(texts, labels)\n",
    "\n",
    "\n",
    "# создайте объект dataloader с batch_size=3 и реализованным collate_fn\n",
    "dataloader = DataLoader(dataset, batch_size=3, collate_fn=collate_fn)\n",
    "\n",
    "# вывод результатов\n",
    "for batch in dataloader:\n",
    "    print(\"Texts:\\n\", batch['texts'])\n",
    "    print(\"Masks:\\n\", batch['masks'])\n",
    "    print(\"Labels:\", batch['labels'])\n",
    "    print(\"Lengths:\", batch['lengths'])\n",
    "    print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-experiments-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
