{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e54f7b",
   "metadata": {},
   "source": [
    "# Encoder-decoder: мост от RNN к трансформерам\n",
    "\n",
    "Прогресс не стоит на месте и сейчас почти все лучшие модели машинного обучения используют под капотом другую архитектуру — transformer. \n",
    "\n",
    "Прежде чем приступить к разбору архитектуры трансформера, важно понять, как он появился. На появление трансформера повлияли три ключевые вещи:\n",
    "\n",
    "- seq2seq-задача;\n",
    "\n",
    "- encoder-decoder архитектура;\n",
    "\n",
    "- механизм внимания в encoder-decoder архитектуре.\n",
    "\n",
    "## Seq2Seq-задачи\n",
    "\n",
    "В машинном обучении особое место занимают задачи, где и вход, и выход являются последовательностями переменной длины. Такие задачи называются sequence-to-sequence (seq2seq). Это важный класс задач, для которых не подходит простая классификация или регрессия, поскольку длина выходной последовательности заранее неизвестна и зависит от входа.\n",
    "\n",
    "Примеры seq2seq-задач:\n",
    "\n",
    "- Машинный перевод: вход — «I am a student», выход — «Я студент».\n",
    "  \n",
    "- Распознавание речи (speech-to-text): аудиосигнал (последовательность признаков) превращается в текст.\n",
    "  \n",
    "- Обобщение текста (summarization): вход — длинный текст, выход — краткое содержание.\n",
    "  \n",
    "- Генерация кода: вход — описание задачи, выход — сгенерированный код.\n",
    "  \n",
    "- Диалоговые агенты: вход — вопрос, выход — ответ.\n",
    "\n",
    "Это далеко не все возможные примеры, seq2seq-задачи встречаются повсеместно.\n",
    "\n",
    "Важно отметить, что не всякая NLP-задача является seq2seq. Например, классификация отзывов — это задача с фиксированным выходом (метка класса), она не требует генерации последовательности. Переменная длина и входа, и выхода — это единственное необходимое и достаточное условие, по которому можно определить seq2seq-задачу.\n",
    "\n",
    "## Как выглядит датасет в seq2seq\n",
    "\n",
    "В отличие от задач классификации, в seq2seq модель обучается по парам: \"input_text\" → \"target_text\".\n",
    "Например:\n",
    "```\n",
    "\"where is the bank?\" → \"wo ist die Bank?\"\n",
    "\"good morning\" → \"guten Morgen\" \n",
    "```\n",
    "На этапе препроцессинга тексты преобразуются в токены:\n",
    "\n",
    "- input_ids — токены входа (например, [\"where\", \"is\", \"the\", \"bank\", \"?\"]);\n",
    "\n",
    "- target_ids — токены выхода (например, [\"wo\", \"ist\", \"die\", \"Bank\", \"?\"]).\n",
    "\n",
    ">Таргет — это тоже последовательность токенов. \n",
    "\n",
    "Поэтому при генерации ответа можно рассматривать задачу как последовательное решение нескольких задач классификации: предсказание первого токена, второго и так далее. \n",
    "\n",
    "Это позволяет использовать функции потерь, как и для обычной классификации. \n",
    "Обычно используется кросс-энтропия.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee3f61c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
