{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055bb041",
   "metadata": {},
   "source": [
    "# Подготовка датасета для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a395cce",
   "metadata": {},
   "source": [
    "Мультимодальные данные — это информация из разных источников в разных форматах. В машинном обучении под мультимодальностью обычно понимают комбинацию как минимум двух типов данных (модальностей): текста, изображений, аудио или видео. \n",
    "\n",
    "Мультимодальные системы умеют извлекать и комбинировать признаки из разных модальностей, что помогает делать более точные предсказания по сравнению с унимодальными подходами."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b2a4a",
   "metadata": {},
   "source": [
    "Особенно интересны задачи кросс-модального поиска, где нужно находить соответствия между разными модальностями. Например, поиск изображений по текстовому запросу или генерация текстовых описаний по изображению. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4890accc",
   "metadata": {},
   "source": [
    "При работе с мультимодальными данными можно столкнуться с несколькими сложностями:\n",
    "\n",
    "- Разные модальности имеют разную природу и структуру. Например, изображения представляют собой регулярные сетки пикселей, а текст — это последовательность символов или слов.\n",
    "    \n",
    "- Модальности могут иметь разную информативность для конкретной задачи. В некоторых случаях одна модальность может доминировать, а другая — лишь незначительно улучшать результаты.\n",
    "    \n",
    "- Возникает проблема согласования временных или пространственных масштабов, особенно при работе с видео и аудио.\n",
    "\n",
    "Напишем базовый загрузчик данных для обучения мультимодальной модели, работающей с текстами и картинками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc163076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_dl_experiments import settings\n",
    "\n",
    "root_dataset_path: str = settings.SOURCE_PATH + \"datasets/multimodal_practice/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4accb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import timm\n",
    "from transformers import AutoTokenizer\n",
    "import torchvision.transforms.v2 as T \n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, text_model, image_model, transforms):\n",
    "        self.df = df\n",
    "        self.image_cfg = timm.get_pretrained_cfg(image_model)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(text_model)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, \"text\"]\n",
    "        label = self.df.loc[idx, \"label\"]\n",
    "\n",
    "        img_path = self.df.loc[idx, \"image_path\"]\n",
    "        image = Image.open(f\"data/images/{img_path}\").convert('RGB')\n",
    "        image = self.transforms(image=np.array(image))[\"image\"]\n",
    "\n",
    "        return {\"label\": label, \"image\": image, \"text\": text}\n",
    "\n",
    "\n",
    "def collate_fn(batch, tokenizer):\n",
    "    texts = [item[\"text\"] for item in batch]\n",
    "    images = torch.stack([item[\"image\"] for item in batch])\n",
    "    labels = torch.LongTensor([item[\"label\"] for item in batch])\n",
    "\n",
    "    tokenized_input = tokenizer(texts,\n",
    "                                return_tensors=\"pt\",\n",
    "                                padding=\"max_length\",\n",
    "                                truncation=True)\n",
    "    return {\n",
    "        \"label\": labels,\n",
    "        \"image\": images,\n",
    "        \"input_ids\": tokenized_input[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized_input[\"attention_mask\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294857c0",
   "metadata": {},
   "source": [
    "Однако этому датасету не хватает важной составляющей для обучения — аугментаций. Также токенизация текстов в датасете выполняется по отдельности, поэтому имеет смысл вынести всю обработку в collate_fn, которую можно затем передать в DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16fbf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model = \"bert-base-uncased\"\n",
    "image_model = 'tf_efficientnet_b0'\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model)\n",
    "cfg = timm.get_pretrained_cfg(image_model)\n",
    "\n",
    "transforms = A.Compose(\n",
    "    [\n",
    "        A.SmallestMaxSize(max_size=max(cfg.input_size[1], cfg.input_size[2]),\n",
    "                          p=1.0),\n",
    "        A.RandomCrop(height=cfg.input_size[1], width=cfg.input_size[2], p=1.0),\n",
    "        A.Affine(scale=(0.8, 1.2),\n",
    "                 rotate=(-15, 15),\n",
    "                 translate_percent=(-0.1, 0.1),\n",
    "                 shear=(-10, 10),\n",
    "                 fill=0,\n",
    "                 p=0.8),\n",
    "        A.CoarseDropout(num_holes_range=(2, 8),\n",
    "                        hole_height_range=(int(0.07 * cfg.input_size[1]),\n",
    "                                           int(0.15 * cfg.input_size[1])),\n",
    "                        hole_width_range=(int(0.1 * cfg.input_size[2]),\n",
    "                                          int(0.15 * cfg.input_size[2])),\n",
    "                        fill=0,\n",
    "                        p=0.5),\n",
    "        A.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.7),\n",
    "        A.Normalize(mean=cfg.mean, std=cfg.std)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565a5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(root_dataset_path+\"items.csv\")\n",
    "ds = MultimodalDataset(df,\n",
    "                       text_model=text_model,\n",
    "                       image_model=image_model,\n",
    "                       transforms=transforms)\n",
    "\n",
    "loader = DataLoader(ds,\n",
    "                    batch_size=1,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=partial(collate_fn, tokenizer=tokenizer)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb3d039",
   "metadata": {},
   "source": [
    "## Текстовые аугментации\n",
    "Они служат тем же целям, что картиночные, то есть помогают разнообразить выборку и улучшить сходимость модели. Их можно разделить на следующие группы:\n",
    "\n",
    "- Замена символов — случайные замены/перестановки символов в словах для имитации опечаток.\n",
    "    \n",
    "- Замена слов — случайные перестановки соседних слов/предложений или замена слов на синонимы.\n",
    "\n",
    "- Обратный перевод — целый текст переводится на какой-то другой язык с помощью сторонней модели, а затем обратно на исходный.\n",
    "\n",
    "Такие текстовые аугментации в некоторых случаях могут заметно обогатить изначально небольшой датасет без особых дополнительных усилий по поиску новых данных. Но и ограничения у них есть, ведь если использовать их слишком часто, они могут сильно исказить исходный смысл предложений."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb2990b",
   "metadata": {},
   "source": [
    "Текстовые аугментации можно вычислять как в момент обучения с целью экономии памяти, так и непосредственно в офлайне перед обучением. Последнее особенно актуально, если используется только один вид текстовых аугментаций (например, обратный перевод).\n",
    "\n",
    "Текстовые аугментации опциональны и менее популярны, чем аугментации картинок. Но их стоит иметь в виду, ведь иногда они могут помочь дотянуть пару процентных пунктов к целевой метрике."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6b312b",
   "metadata": {},
   "source": [
    "# Реализация архитектуры мультимодальной нейросети\n",
    "\n",
    "## Архитектура мультимодальной сети\n",
    "\n",
    "При разработке мультимодальной архитектуры нужно учитывать два основных момента: объединение энкодеров/декодеров разных модальностей и объединение эмбеддингов разных модальностей.\n",
    "Чтобы объединить две модели в одну, достаточно:\n",
    "\n",
    "- Указать их в рамках одного класса nn.Module и проинициализировать.\n",
    "    \n",
    "- В методе forward() рассчитать эмбеддинги для каждой модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "783b0c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 768]), torch.Size([2, 2048]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "\n",
    "class BaseDefaultMultimodalModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_model_name='bert-base-uncased',\n",
    "                 image_model_name='resnet50'):\n",
    "        super().__init__()\n",
    "\n",
    "        # Текстовая ветка\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "\n",
    "        # Визуальная ветка\n",
    "        self.image_model = timm.create_model(\n",
    "            image_model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0  # Возвращаем вектор признаков, а не классы\n",
    "        )\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        # эмбеддинги текста\n",
    "        text_features = self.text_model(**text_input).last_hidden_state[:,  0, :]\n",
    "\n",
    "        # эмбеддинги изображений\n",
    "        image_features = self.image_model(image_input)\n",
    "        return text_features, image_features\n",
    "\n",
    "text_model = 'bert-base-uncased'\n",
    "image_model = 'resnet50'\n",
    "m = BaseDefaultMultimodalModel(text_model_name=text_model,\n",
    "                        image_model_name=image_model)\n",
    "\n",
    "# 2 примера текста и картинки для инференса\n",
    "tk = AutoTokenizer.from_pretrained(text_model)\n",
    "tokenized = tk([\"text\", \"text2\"],\n",
    "               return_tensors=\"pt\",\n",
    "               padding='max_length',\n",
    "               truncation=True)\n",
    "\n",
    "\n",
    "# вместо самой картинки используем случайный тензор правильной размерности\n",
    "# размерность тензора берём из конфига и распаковываем через `*`\n",
    "img = torch.randn(2, *m.image_model.pretrained_cfg[\"input_size\"])\n",
    "\n",
    "t, i = m(tokenized, img)\n",
    "t.shape, i.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587676ba",
   "metadata": {},
   "source": [
    "## Объединение эмбеддингов\n",
    "\n",
    "Варианты объединения: \n",
    "\n",
    "- После расчёта эмбеддингов по каждой модальности конкатенируем их, а поверх используем классификационную голову (например, MLP-слой после).\n",
    "    \n",
    "- Проделаем то же самое, что и выше, но поэлементно перемножаем эмбеддинги.\n",
    "    \n",
    "- Кросс-модальное внимание.\n",
    "\n",
    "Во всех случаях исходные эмбеддинги из моделей, как правило, приводятся к одной, меньшей размерности. Это решает две задачи:\n",
    "\n",
    "- Позволяет пропорционально учитывать каждую из модальностей в дальнейшем предсказании.\n",
    "    \n",
    "- Способствует более устойчивому обучению — в случае больших размерностей эмбеддингов модели часто переобучаются.\n",
    "\n",
    "Выбор между конкретным вариантом часто основывается на типе и сложности задачи: если в задаче важно взаимодополнение модальностей, используют кросс-модальное внимание, в случае обычной классификации иногда достаточно перемножения/конкатенации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e374b23",
   "metadata": {},
   "source": [
    "Давайте посмотрим на реализацию слияния эмбеддингов через поэлементное перемножение. Для этого адаптируем код BaseMultimodalModel, добавив в него проекции эмбеддинга каждой модальности в пространство меньшей размерности — 256 — и визуализируем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерности проекций эмбеддингов: torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BaseMultiplyMultimodalModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_model_name='bert-base-uncased',\n",
    "                 image_model_name='resnet50'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.image_model = timm.create_model(\n",
    "            image_model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0 \n",
    "        )\n",
    "\n",
    "        # проецируем каждый из векторов в размерность 256\n",
    "        # входную размерность линейного слоя задаём динамически из конфигов моделей\n",
    "        self.text_proj = nn.Linear(self.text_model.config.hidden_size, 256)\n",
    "        self.image_proj = nn.Linear(self.image_model.num_features, 256)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        text_features = self.text_model(**text_input).last_hidden_state[:,  0, :]\n",
    "        image_features = self.image_model(image_input)\n",
    "\n",
    "        # вычисляем проекции эмбеддингов и объединяем их\n",
    "        text_emb = self.text_proj(text_features)\n",
    "        image_emb = self.image_proj(image_features)\n",
    "        \n",
    "        print(\"Размерности проекций эмбеддингов:\",text_emb.shape, image_emb.shape)\n",
    "\n",
    "        fused_emb = text_emb * image_emb\n",
    "        return fused_emb\n",
    "\n",
    "text_model = 'bert-base-uncased'\n",
    "image_model = 'resnet50'\n",
    "m = BaseMultiplyMultimodalModel(text_model_name=text_model,\n",
    "                        image_model_name=image_model)\n",
    "\n",
    "# 2 примера текста и картинки для инференса\n",
    "tk = AutoTokenizer.from_pretrained(text_model)\n",
    "tokenized = tk([\"text\", \"text2\"],\n",
    "               return_tensors=\"pt\",\n",
    "               padding='max_length',\n",
    "               truncation=True)\n",
    "\n",
    "img = torch.randn(2, *m.image_model.pretrained_cfg[\"input_size\"])\n",
    "\n",
    "emb = m(tokenized, img)\n",
    "emb.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1cc9a",
   "metadata": {},
   "source": [
    "Вы получили каркас для решения мультимодальной задачи — эмбеддинги спроецированы в одно пространство и объединены. Как правило, для проецирования используют числа меньшей размерности, чем каждый из эмбеддингов модальностей, являющиеся степенью двойки (256, 512, 1024, ..). Однако могут быть и другие значения, которые можно многократно разделить на 2 (например, 768). Это связано с особенностями хранения и обработки тензоров на GPU — такие размеры позволяют эффективнее использовать память и вычислительные ресурсы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9164e",
   "metadata": {},
   "source": [
    "Адаптируем класс модели, эмбеддинги сливаются через конкатинацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4f62603",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import timm\n",
    "\n",
    "\n",
    "class BaseConcatMultimodalModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_model_name='bert-base-uncased',\n",
    "                 image_model_name='resnet50',\n",
    "                 emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.image_model = timm.create_model(\n",
    "            image_model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0 \n",
    "        )\n",
    "\n",
    "        self.text_proj = nn.Linear(self.text_model.config.hidden_size, emb_dim)\n",
    "        self.image_proj = nn.Linear(self.image_model.num_features, emb_dim)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        text_features = self.text_model(**text_input).last_hidden_state[:,  0, :]\n",
    "        image_features = self.image_model(image_input)\n",
    "\n",
    "        text_emb = self.text_proj(text_features)\n",
    "        image_emb = self.image_proj(image_features)\n",
    "\n",
    "        fused_emb = torch.cat([text_emb, image_emb], dim=1)\n",
    "        return fused_emb\n",
    "\n",
    "\n",
    "text_model = 'bert-base-uncased'\n",
    "image_model = 'resnet50'\n",
    "m = BaseConcatMultimodalModel(text_model_name=text_model,\n",
    "                        image_model_name=image_model)\n",
    "\n",
    "\n",
    "# 2 примера текста и картинки для инференса\n",
    "tk = AutoTokenizer.from_pretrained(text_model)\n",
    "tokenized = tk([\"text\", \"text2\"],\n",
    "               return_tensors=\"pt\",\n",
    "               padding='max_length',\n",
    "               truncation=True)\n",
    "\n",
    "img = torch.randn(2, *m.image_model.pretrained_cfg[\"input_size\"])\n",
    "emb = m(tokenized, img)\n",
    "emb.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b5316",
   "metadata": {},
   "source": [
    "## Крос-модальное внимание:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ef75c",
   "metadata": {},
   "source": [
    "В самом простом варианте реализации кросс-модальное внимание — это просто attention-слой, в котором в качестве запросов выступают значения одной модальности, а в качестве ключей и значений — другой. \n",
    "\n",
    "Есть и другие варианты реализации, но мы остановимся на базовом.\n",
    "\n",
    "\n",
    "Посмотрите на него в коде, в котором BaseMultimodalModel используется как часть общей модели. Инициализируем и добавим слой MultiheadAttention, в который передадим полученные эмбеддинги картинок и текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89bc1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMultimodalModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 text_model_name='bert-base-uncased',\n",
    "                 image_model_name='resnet50',\n",
    "                 emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        self.image_model = timm.create_model(\n",
    "            image_model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0 \n",
    "        )\n",
    "\n",
    "        # проецируем каждый из векторов в размерность 256\n",
    "        # входную размерность линейного слоя задаём динамически из конфигов моделей\n",
    "        self.text_proj = nn.Linear(self.text_model.config.hidden_size, emb_dim)\n",
    "        self.image_proj = nn.Linear(self.image_model.num_features, emb_dim)\n",
    "\n",
    "    def forward(self, text_input, image_input):\n",
    "        text_features = self.text_model(**text_input).last_hidden_state[:,  0, :]\n",
    "        image_features = self.image_model(image_input)\n",
    "\n",
    "        # вычисляем проекции эмбеддингов и объединяем их\n",
    "        text_emb = self.text_proj(text_features)\n",
    "        image_emb = self.image_proj(image_features)\n",
    "        \n",
    "        print(\"Размерности проекций эмбеддингов:\",text_emb.shape, image_emb.shape)\n",
    "\n",
    "        return text_emb, image_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5f81fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерности проекций эмбеддингов: torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    def __init__(self, text_model_name='bert-base-uncased', image_model_name='resnet50'):\n",
    "        super().__init__()\n",
    "        # Инициализация базовых моделей\n",
    "        self.base_model = BaseMultimodalModel(text_model_name, image_model_name)\n",
    "        \n",
    "        # Механизм внимания\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=self.base_model.emb_dim, \n",
    "            num_heads=4)\n",
    "        \n",
    "    def forward(self, text_input, image_input):\n",
    "        # Получаем эмбеддинги модальностей\n",
    "        text_emb, image_emb = self.base_model(text_input, image_input)\n",
    "        \n",
    "        # Подготовка для внимания - добавляем размерность последовательности\n",
    "        # так как она необходима для вычислений в MultiheadAttention\n",
    "        text_emb = text_emb.unsqueeze(0)  # [1, batch_size, emb_dim]\n",
    "        image_emb = image_emb.unsqueeze(0)  # [1, batch_size, emb_dim]\n",
    "        \n",
    "        # Текст как запрос, изображение как ключ/значение\n",
    "        attended_emb, _ = self.cross_attn(\n",
    "            query=text_emb,\n",
    "            key=image_emb,\n",
    "            value=image_emb\n",
    "        )\n",
    "        \n",
    "        # Возвращаем эмбеддинги без первой размерности\n",
    "        # так как она нам нужна только в шаге с расчётом attetion\n",
    "        return attended_emb.squeeze(0)\n",
    "\n",
    "\n",
    "cam = CrossAttentionModel()\n",
    "out = cam(tokenized, img)\n",
    "out.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a29e9",
   "metadata": {},
   "source": [
    "Добавьте в архитектуру CrossAttentionModel классификационный слой — двухслойный MLP с уменьшением размерности в два раза. Используйте дропаут и нормализации. Метод forward() должен возвращать логиты предсказаний. Количество классов num_classes установите равным 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1b35fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 text_model_name='bert-base-uncased', \n",
    "                 image_model_name='resnet50', \n",
    "                 num_classes=2):\n",
    "        super().__init__()\n",
    "        self.base_model = BaseMultimodalModel(text_model_name, image_model_name)\n",
    "        emb_dim = self.base_model.emb_dim\n",
    "        \n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=emb_dim, \n",
    "            num_heads=4)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim // 2),    # Уменьшение размерности\n",
    "            nn.LayerNorm(emb_dim // 2),          # Нормализация\n",
    "            nn.ReLU(),                           # Активация\n",
    "            nn.Dropout(0.15),                    # Регуляризация\n",
    "            nn.Linear(emb_dim // 2, num_classes) # Финальный слой\n",
    "        )\n",
    "        \n",
    "    def forward(self, text_input, image_input):\n",
    "        text_emb, image_emb = self.base_model(text_input, image_input)\n",
    "        \n",
    "        text_emb = text_emb.unsqueeze(0)  \n",
    "        image_emb = image_emb.unsqueeze(0)  \n",
    "        \n",
    "        attended, _ = self.cross_attn(\n",
    "            query=text_emb,\n",
    "            key=image_emb,\n",
    "            value=image_emb\n",
    "        )\n",
    "\n",
    "        # логиты предсказаний\n",
    "        logits = self.classifier(attended.squeeze(0))\n",
    "        return logits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd8cdb4",
   "metadata": {},
   "source": [
    "# Реализация пайплайна обучения мультимодальной нейросети и замера качества"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36d2f6a",
   "metadata": {},
   "source": [
    "## Пайплайн обучения мультимодальной нейросети\n",
    "\n",
    "Обучение мультимодальной модели мало чем отличается от знакомых вам одномодальных архитектур. Вам нужно:\n",
    "\n",
    "- Создать загрузчики и обёртки для датасетов.\n",
    "    \n",
    "- Сформировать модуль с архитектурой модели.\n",
    "    \n",
    "- Реализовать цикл обучения и валидации.\n",
    "\n",
    "Кроме данных и итоговой модели, для экспериментов необходим конфиг запуска — набор параметров, описывающих архитектуру, гиперпараметры и обработку данных.\n",
    "\n",
    "Существует бесконечное множество форматов представления данных файлов конфигурации — от JSON-файла до Python-класса.\n",
    "\n",
    "В качестве примера реализуем свой конфиг Config для запуска обучения через Python-класс. Передадим в него ключевые параметры, определённые в предыдущих уроках, и добавим полезные настройки:\n",
    "\n",
    "- имена моделей;\n",
    "    \n",
    "- размер батча;\n",
    "    \n",
    "- число эпох обучения;\n",
    "    \n",
    "- количество классов в задаче (если мы говорим о задаче классификации);\n",
    "    \n",
    "- численные значения, определяющие обучения PyTorch модулей (например, dropout);\n",
    "    \n",
    "- размерность эмбеддингов для проекции;\n",
    "    \n",
    "- learning rate (LR).\n",
    "\n",
    "Отдельно отметим нюансы, связанные со скоростью обучения.\n",
    "При тюнинге LR обычно ниже, чем при обучении с нуля. У разных архитектур свои оптимальные значения: трансформеры для NLP часто требуют LR на порядок меньше, чем CV архитектуры вроде EfficientNet. Для MLP‑головы тоже можно варьировать LR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e25754",
   "metadata": {},
   "source": [
    "## 1.Построение универсального конфига."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681bb21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Модели\n",
    "    TEXT_MODEL_NAME = \"bert-base-uncased\"\n",
    "    IMAGE_MODEL_NAME = \"resnet50\"\n",
    "    \n",
    "    # Какие слои размораживаем - совпадают с неймингом в моделях\n",
    "    TEXT_MODEL_UNFREEZE = \"encoder.layer.11|pooler\"  \n",
    "    IMAGE_MODEL_UNFREEZE = \"layer.3|layer.4\"  \n",
    "    \n",
    "    # Гиперпараметры\n",
    "    BATCH_SIZE = 32\n",
    "    TEXT_LR = 3e-5\n",
    "    IMAGE_LR = 1e-4\n",
    "    CLASSIFIER_LR = 5e-4\n",
    "    EPOCHS = 10\n",
    "    DROPOUT = 0.3\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_CLASSES = 5\n",
    "    \n",
    "    # Пути\n",
    "    TRAIN_DF_PATH = \"path/train.csv\"\n",
    "    VAL_DF_PATH = \"path/val.csv\"\n",
    "    SAVE_PATH = \"best_model.pth\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74fbba",
   "metadata": {},
   "source": [
    "Оптимизаторы `PyTorch` могут одновременно обучать разные параметры модели с разными настройками. Например, для абстрактной модели `model`, состоящей из двух частей — `embedder` и `classifier`, можно передать в любой оптимизатор `PyTorch` настройки params (веса подмодуля) и `lr` (`learning rate`, используемый для этих весов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98d8c6",
   "metadata": {},
   "source": [
    "```py\n",
    "# на примере оптимизатора AdamW\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {'params': model.embedder.parameters(), 'lr': 0.001},\n",
    "    {'params': model.classifier.parameters(), 'lr': 0.0001},\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f9481",
   "metadata": {},
   "source": [
    "Кроме того, не всегда нужно обучать всю модель с нуля — иногда достаточно заморозить часть слоёв или, наоборот, всю модель, оставив для обучения только классификатор. \n",
    "\n",
    "Набор слоёв для разморозки можно указать в конфиге модели текстовым полем (предварительно стоит вывести структуру модели и выбрать слои по их именам). Обычно при частичной разморозке выбирают слои, идущие подряд — с первого по последний без пропусков. Например, в трёхслойной сети нельзя разморозить только первый и третий слои."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e07ef",
   "metadata": {},
   "source": [
    "Для удобства разморозки можно объединить этот процесс в функцию set_requires_grad, которая принимает модель (или её часть, nn.Module) и unfreeze_pattern — текстовые имена слоёв для разморозки (или несколько имён, разделённых |, как в Config выше, например: \"layer.3|layer.4\" ).\n",
    "\n",
    "Итерируясь по параметрам и их именам, функция размораживает выбранные слои. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a9876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разморожен слой: layer4.0.conv1.weight\n",
      "Разморожен слой: layer4.0.bn1.weight\n",
      "Разморожен слой: layer4.0.bn1.bias\n",
      "Разморожен слой: layer4.0.conv2.weight\n",
      "Разморожен слой: layer4.0.bn2.weight\n",
      "Разморожен слой: layer4.0.bn2.bias\n",
      "Разморожен слой: layer4.0.conv3.weight\n",
      "Разморожен слой: layer4.0.bn3.weight\n",
      "Разморожен слой: layer4.0.bn3.bias\n",
      "Разморожен слой: layer4.0.downsample.0.weight\n",
      "Разморожен слой: layer4.0.downsample.1.weight\n",
      "Разморожен слой: layer4.0.downsample.1.bias\n",
      "Разморожен слой: layer4.1.conv1.weight\n",
      "Разморожен слой: layer4.1.bn1.weight\n",
      "Разморожен слой: layer4.1.bn1.bias\n",
      "Разморожен слой: layer4.1.conv2.weight\n",
      "Разморожен слой: layer4.1.bn2.weight\n",
      "Разморожен слой: layer4.1.bn2.bias\n",
      "Разморожен слой: layer4.1.conv3.weight\n",
      "Разморожен слой: layer4.1.bn3.weight\n",
      "Разморожен слой: layer4.1.bn3.bias\n",
      "Разморожен слой: layer4.2.conv1.weight\n",
      "Разморожен слой: layer4.2.bn1.weight\n",
      "Разморожен слой: layer4.2.bn1.bias\n",
      "Разморожен слой: layer4.2.conv2.weight\n",
      "Разморожен слой: layer4.2.bn2.weight\n",
      "Разморожен слой: layer4.2.bn2.bias\n",
      "Разморожен слой: layer4.2.conv3.weight\n",
      "Разморожен слой: layer4.2.bn3.weight\n",
      "Разморожен слой: layer4.2.bn3.bias\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "\n",
    "def set_requires_grad(module, unfreeze_pattern, verbose=False):\n",
    "    # Если пустая строка - замораживаем все\n",
    "    if len(unfreeze_pattern) == 0:\n",
    "        for _, param in module.named_parameters():\n",
    "            param.requires_grad = False\n",
    "        return\n",
    "\n",
    "    # разбиваем все слои\n",
    "    pattern = unfreeze_pattern.split(\"|\")\n",
    "\n",
    "    # Проходим по всем слоям и ищем совпадения любого\n",
    "    # слоя из `pattern` с текущим именем слоя\n",
    "    for name, param in module.named_parameters():\n",
    "        if any([name.startswith(p) for p in pattern]):\n",
    "            param.requires_grad = True\n",
    "            if verbose:\n",
    "                print(f\"Разморожен слой: {name}\")\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "# Разморозка последнего слоя resnet50\n",
    "image_model = timm.create_model(\n",
    "            'resnet50',\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        )\n",
    "\n",
    "set_requires_grad(image_model, unfreeze_pattern=\"layer4\", verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d1ced",
   "metadata": {},
   "source": [
    "Для метрик достаточно полезно использовать torchmetrics \n",
    "Важно перенести метрику на верный device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c81f03c",
   "metadata": {},
   "source": [
    "```py\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# порог по умолчанию равен 0.5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "f1 = torchmetrics.F1Score(task=\"binary\", num_classes=2).to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742f0c7",
   "metadata": {},
   "source": [
    "Чтобы вычислить метрику и сохранить промежуточный результат, необходимо передать в неё тензоры с предсказаниями и истинные метки объектов. \n",
    "\n",
    "Чтобы агрегировать результаты между батчами, необходимо вызвать метод compute(), чтобы обнулить сохранённые результаты — метод reset()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c05a6",
   "metadata": {},
   "source": [
    "```py\n",
    "# обнуляем метрику, можно вызывать в любой момент\n",
    "f1.reset()\n",
    "\n",
    "for i in [0, 1, 1, 0]:\n",
    "    # имимтируем разные метрики в батчах\n",
    "    score = f1(preds=torch.Tensor([0.1, 0.6, i, 0.2]),\n",
    "               target=torch.Tensor([0, 1, 1, 1]))\n",
    "    print(score)\n",
    "\n",
    "# агрегируем результаты\n",
    "score = f1.compute()\n",
    "print(score)\n",
    "```\n",
    "\n",
    "```py\n",
    "batch score: tensor(0.5000) \n",
    "batch score: tensor(0.8000) \n",
    "batch score: tensor(0.8000) \n",
    "batch score: tensor(0.5000) \n",
    "epoch score: tensor(0.6667)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9254dee",
   "metadata": {},
   "source": [
    "Мультимодальные модели с точки зрения кода не сильно отличаются от обычных одномодальных архитектур. Основные отличия заключаются в необходимости слияния признаков разных модальностей, а также подбора качественного мультимодального датасета. \n",
    "\n",
    "В задачах классификации становится возможным дополнительно оценить влияние каждой из модальностей на ответ модели, путём маскирования смежной.\n",
    "\n",
    "Подведём итог всей темы. Вы проделали большой путь: от подготовки данных и аугментаций до сбора мультимодального датасета и обучения модели, которая может обрабатывать картинки и тексты одновременно. \n",
    "\n",
    "На реальном кейсе разобрались в графиках обучения моделей и сделали выводы о том, как можно оценивать влияние различных признаков на целевую задачу.\n",
    "\n",
    "Эти знания — крепкая основа для решения базовых задач Deep Learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-experiments-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
