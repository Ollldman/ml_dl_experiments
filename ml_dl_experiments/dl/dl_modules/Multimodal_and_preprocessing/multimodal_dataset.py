from typing import Any, Callable, Dict, List, Optional, Union
import os

import torch
from torch.utils.data import Dataset
import pandas as pd
from PIL import Image
import numpy as np
from transformers import AutoTokenizer, PreTrainedTokenizerBase
import timm
from timm.models import PretrainedCfg
import albumentations as A
from torchvision.transforms import Compose


class MultimodalDataset(Dataset):
    """A PyTorch Dataset for multimodal (text + image) classification tasks.

    This dataset loads image-text pairs from a pandas DataFrame, applies
    image transformations, and prepares raw text for tokenization
    (tokenization is deferred to the collate function).

    Expected DataFrame columns:
        - 'text' (str): Input text.
        - 'label' (int): Class label.
        - 'image_path' (str): Relative path to image file (relative to `dataset_root_path/images/`).

    Args:
        df (pd.DataFrame): Input DataFrame containing text, label, and image paths.
        text_model (str): Hugging Face model name for tokenizer (e.g., 'bert-base-uncased').
        image_model (str): TIMM model name to infer preprocessing config (e.g., 'resnet50').
        transforms (Union[A.Compose, Compose, Callable]): Image transformation pipeline.
            Supports both Albumentations and Torchvision transforms.
        dataset_root_path (str): Root directory of the dataset. Images are expected under
            `{dataset_root_path}/images/`.
    """

    def __init__(
        self,
        df: pd.DataFrame,
        text_model: str,
        image_model: str,
        transforms: Union[A.Compose, Compose, Callable[[np.ndarray], torch.Tensor]],
        dataset_root_path: str,
    ) -> None:
        self.df = df.reset_index(drop=True)
        self.image_cfg: PretrainedCfg = timm.get_pretrained_cfg(image_model) # type:ignore
        self.tokenizer: PreTrainedTokenizerBase = AutoTokenizer.from_pretrained(text_model)
        self.transforms = transforms
        self.dataset_root_path = dataset_root_path

    def __len__(self) -> int:
        """Returns the number of samples in the dataset."""
        return len(self.df)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """Loads and preprocesses a single image-text pair.

        Args:
            idx (int): Index of the sample.

        Returns:
            Dict[str, Any]: A dictionary with keys:
                - 'text' (str): Raw input text.
                - 'label' (int): Ground truth class label.
                - 'image' (np.ndarray or torch.Tensor): Transformed image.
        """
        row = self.df.iloc[idx]
        text: str = row["text"]
        label: int = row["label"]
        img_path: str = row["image_path"]

        # Load image
        full_img_path = os.path.join(self.dataset_root_path, "images", img_path)
        image_pil = Image.open(full_img_path).convert("RGB")
        image_np = np.array(image_pil)

        # Apply transforms
        if isinstance(self.transforms, A.Compose):
            # Albumentations expects and returns numpy arrays
            transformed = self.transforms(image=image_np)
            image = transformed["image"]
        else:
            # Torchvision expects PIL or tensor; we pass numpy â†’ it auto-converts
            image = self.transforms(image_np)

        return {
            "text": text,
            "label": label,
            "image": image,
        }


def collate_fn(
    batch: List[Dict[str, Any]],
    tokenizer: PreTrainedTokenizerBase,
    max_length: Optional[int] = None,
) -> Dict[str, torch.Tensor]:
    """Collates a batch of samples into tensors for multimodal training.

    Tokenizes text, stacks images, and converts labels to tensors.

    Args:
        batch (List[Dict[str, Any]]): List of samples from MultimodalDataset.
        tokenizer (PreTrainedTokenizerBase): Hugging Face tokenizer.
        max_length (Optional[int]): Max sequence length for tokenization.
            If None, tokenizer's default is used.

    Returns:
        Dict[str, torch.Tensor]: Batch dictionary with keys:
            - 'label': (B,)
            - 'image': (B, C, H, W)
            - 'input_ids': (B, L)
            - 'attention_mask': (B, L)
    """
    texts: List[str] = [item["text"] for item in batch]
    images: List[torch.Tensor] = [item["image"] for item in batch]
    labels: List[int] = [item["label"] for item in batch]

    # Stack images (assumes all have same shape after transforms)
    image_tensor = torch.stack(images, dim=0)  # (B, C, H, W)
    label_tensor = torch.LongTensor(labels)    # (B,)

    # Tokenize texts
    tokenized = tokenizer(
        texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_length,
    )

    return {
        "label": label_tensor,
        "image": image_tensor,
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
    } # type:ignore