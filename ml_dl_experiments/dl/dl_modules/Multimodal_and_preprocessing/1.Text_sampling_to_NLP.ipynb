{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233539e3",
   "metadata": {},
   "source": [
    "# Токенизация текстов в деталях"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc6c9b",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "**Term Frequency-Inverse Document Frequency**\n",
    "\n",
    "Каждый вектор будет характеризовать соответствующий документ на основе встречаемости слов, а каждый элемент вектора — соответствовать какому-то слову из словаря слов всего корпуса. \n",
    "\n",
    "Получается матрица размерности `Число документов × Число слов в документах`. Каждому слову присвоен счётчик. Чем ближе число в ячейке к 0, тем менее важно слово в контексте этого документа. `tf-idf = 0` означает, что слово не встречается в тексте вообще.\n",
    "\n",
    "$$\n",
    "TFIDF = TF * IDF\n",
    "$$\n",
    "\n",
    "$$\n",
    "TF = \\frac{WordCount(w,d)}{Length(d)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "IDF(w,C) = lg\\frac{Size(C)}{DocCount(w, C)}\n",
    "$$\n",
    "\n",
    "- TF, Term Frequency — частота слова в конкретном документе.\n",
    "\n",
    "- IDF, Inverse Document Frequency — обратная частота встречаемости слова среди документов.\n",
    "  \n",
    "- TFIDF — число, описывающее важность отдельно взятого слова для конкретного документа.\n",
    "\n",
    "- DocCount — число документов в корпусе С, где встречается слово w;\n",
    "    \n",
    "- Size(С) — общее число документов в корпусе C.\n",
    "\n",
    "Вернёмся к примеру с генетикой. Пусть есть 20 документов про генетику и в каждом из них употребили слово «ген» по 1 разу. При этом слово «мышь» употребили только в одном документе (X) из 1000 слов. Посчитаем для документа X TF_IDF для слов «мышь» и «ген»:\n",
    "```\n",
    "TF_ген = 1 / 1000 = 0.001\n",
    "TF_мышь = 1 / 1000 = 0.001\n",
    "\n",
    "IDF_ген = log(20 / 20) = 0\n",
    "IDF_мышь = log(20 / 1) =~ 1.301\n",
    "\n",
    "\n",
    "TF_IDF_ген = 0.001 * 0 = 0\n",
    "TF_IDF_мышь = 0.001 * 1.301 = 0.001301\n",
    "```\n",
    "Если слово не характерно для корпуса текстов, то, скорее всего, на него стоит обратить больше внимания."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c48b3",
   "metadata": {},
   "source": [
    "В более общем случае метод TF-IDF работает не только с отдельными словами, но и с n-gram'ами:\n",
    "\n",
    "***N-gram — последовательность из N-символов или слов, идущих по порядку, получаемых путём токенизации. Словарь в TF-IDF будет состоять из N-gram***\n",
    "\n",
    "Пример:\n",
    "```\n",
    "text = 'мама мыла раму'\n",
    "\n",
    "# 1-gram, по словам ИЛИ  word-level токенизация\n",
    "'мама', мыла', 'раму' # единица словаря в этом случае - 1 слово\n",
    "\n",
    "# 2-gram, по словам\n",
    "'мама мыла', 'мыла раму' # единица словаря - 2 слова\n",
    "\n",
    "# 2-gram, посимвольно или  character-level токенизация\n",
    "'ма', 'ам', 'а ', ' м', 'мы', 'ыл', 'ла', ' р', 'ра', 'му' # обратите внимание на пробелы в составе n-gram \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acd08a",
   "metadata": {},
   "source": [
    "## Предобработка корпуса текстов:\n",
    "\n",
    "Задача токенизации — разбить текст на отдельные элементы, которые в дальнейшем будут использованы для анализа. Алгоритмами, выполняющими разбиение, то есть токенизаторами, могут быть:\n",
    "\n",
    "- Регулярные выражения.\n",
    "   \n",
    "- Предобученные словари, построенные на большом количестве документов и выучившие наиболее частотное для конкретного языка совместное расположение символов в текстах.\n",
    "\n",
    "Предобученные токенизаторы включены во многие библиотеки по работе с естественным языком, такие как NLTK (Natural Language ToolKit), spacy и transformers из экосистемы huggingface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174eeaf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from ml_dl_experiments import settings\n",
    "\n",
    "with open(settings.SOURCE_PATH + \"datasets/tiny_shakespear.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d402cf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['First',\n",
       " 'Citizen',\n",
       " 'Before',\n",
       " 'we',\n",
       " 'proceed',\n",
       " 'any',\n",
       " 'further',\n",
       " 'hear',\n",
       " 'me',\n",
       " 'speak']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# регулярное выражение - оставляет только слова длиной  > 1 символа без пунктуации\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "re.findall(pattern, text[:60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea483f7",
   "metadata": {},
   "source": [
    "Токенизируем этот же фрагмент с помощью разных библиотек и сравним результаты. Используем библиотеки NLTK, transformers, а также библиотеку spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b624250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ollldman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ollldman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK токены: ['First', 'Citizen', ':', 'Before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.']\n",
      "\n",
      "transformers токены: ['first', 'citizen', ':', 'before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.']\n",
      "\n",
      "spaCy токены: ['First', 'Citizen', ':', '\\n', 'Before', 'we', 'proceed', 'any', 'further', ',', 'hear', 'me', 'speak', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import word_tokenize\n",
    "from transformers import AutoTokenizer\n",
    "import nltk\n",
    "\n",
    "# для некоторых библиотек требуется ручной вызов загрузки\n",
    "# необходимых словарей, модулей. \n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# для spacy загрузим отдельную модель/словарь для английского языка en_core_web_sm    \n",
    "nlp = spacy.load(settings.SOURCE_PATH + \"ml_dl/models/en_core_web_sm-3.8.0\")\n",
    "\n",
    "# в transformers используем предобученный токенизатор от модели для английского языка\n",
    "tokenizer = AutoTokenizer.from_pretrained(settings.SOURCE_PATH + \"ml_dl/models/distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "#в nltk указываем язык, с которым работаем явно\n",
    "tokens_nltk = word_tokenize(text[:60], language='english') \n",
    "\n",
    "# для spacy предварительно оборачиваем текст во внутренний формат\n",
    "# представления документа и используем атрибут .text у токена\n",
    "doc = nlp(text[:60])\n",
    "tokens_spacy = [token.text for token in doc]\n",
    "\n",
    "# токенайзеры transfomers работают с текстами напрямую\n",
    "tokens_trf = tokenizer.tokenize(text[:60])\n",
    "\n",
    "print(\"\\nNLTK токены:\", tokens_nltk)\n",
    "print(\"\\ntransformers токены:\", tokens_trf )\n",
    "print(\"\\nspaCy токены:\", tokens_spacy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5957a",
   "metadata": {},
   "source": [
    "- nltk (word_tokenize) полностью основан на регулярных выражениях.\n",
    "  \n",
    "- Токенизатор из transformers является предобученным и выполняет исключительно разделение на токены по словарю. Он оптимизирован под скорость работы и обработку батчами.\n",
    "   \n",
    "- Токенизатор из spacy также предобучался, но содержит намного больше атрибутов для каждого токена (например, можно вызвать token.like_email или like_url). Часть из них вычисляются по заданным правилам (regex), часть предсохранены для каждого токена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6016f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "токены: ['123', ':', 'hey', 'ds_expert@w.com', ',', 'check', 'findme.com']\n",
      "числа: [123]\n",
      "имейлы: [ds_expert@w.com]\n",
      "ссылки: [findme.com]\n"
     ]
    }
   ],
   "source": [
    "text = \"123: hey ds_expert@w.com , check findme.com\"\n",
    "\n",
    "# преобразуйте текст во внутренний формат spacy\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# проверьте, является ли токен числом, имейлом, ссылкой\n",
    "digits = [token for token in doc if token.is_digit ]\n",
    "emails = [token for token in doc if token.like_email]\n",
    "urls = [token for token in doc if token.like_url]\n",
    "\n",
    "print(\"токены:\", tokens)\n",
    "print(\"числа:\", digits)\n",
    "print(\"имейлы:\", emails)\n",
    "print(\"ссылки:\", urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa9020",
   "metadata": {},
   "source": [
    "Условно токенизаторы можно разделить по назначению:\n",
    "\n",
    "- spacy позволяют решать некоторые NLP-задачи даже без применения модели.\n",
    "  \n",
    "- transformers рассчитаны на быструю токенизацию с минимальными затратами, принося в жертву набор функций.\n",
    "  \n",
    "- nltk — простое разделение на токены регулярками без дополнительных функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7fa4ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 6287), ('and', 5690), ('to', 4934), ('of', 3760), ('you', 3211)]\n",
      "[('fowling', 1), ('weakly', 1), ('drowsiness', 1), ('possesses', 1), ('eyelids', 1)]\n"
     ]
    }
   ],
   "source": [
    "with open(settings.SOURCE_PATH + \"datasets/tiny_shakespear.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "tokens_regex = re.findall(pattern, text.lower())\n",
    "\n",
    "cnt = Counter(tokens_regex)\n",
    "print(cnt.most_common(5))\n",
    "print(cnt.most_common()[-5:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b5c8b",
   "metadata": {},
   "source": [
    "Самые частые токены — это союзы и артикли, которые в общем не несут основной смысловой нагрузки.\n",
    "\n",
    "Такие слова есть в каждом языке, и в контексте NLP их принято называть «стоп-словами». Как правило, при работе с моделями частотности слов они удаляются, так как не полезны для решения конечной задачи и лишь увеличивают размерность словаря.\n",
    "\n",
    "Легко удалить такие слова можно с помощью готового набора стоп-слов. Такие наборы существуют в библиотеках spacy и nltk. Давайте на примере библиотеки nltk посмотрим, как выполнить их обработку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14b83124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thou', 1421), ('thy', 1059), ('king', 925), ('shall', 849), ('thee', 762)]\n",
      "[('fowling', 1), ('weakly', 1), ('drowsiness', 1), ('possesses', 1), ('eyelids', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ollldman/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens_filtered = [t for t in tokens_regex if t not in stop_words] # фильтруем\n",
    "\n",
    "cnt = Counter(tokens_filtered)\n",
    "print(cnt.most_common(5))\n",
    "print(cnt.most_common()[-5:]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b6683",
   "metadata": {},
   "source": [
    "В целом всё получилось, хотя просочились устаревшие формы слов вроде thy и thou. Это говорит о том, что готовые наборы предобработки — хорошо, но всё же стоит валидировать результаты работы глазами, особенно если у вас какая-то нетипичная лексика в тексте."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1b4dd",
   "metadata": {},
   "source": [
    ">***При обучении трансформерных архитектур иногда может потребоваться корректировка исходного текста путём удаления части токенов. Например, обилие гиперссылок в онлайн-чатах может привести к переобучению. При принятии решения об удалении токенов в каждом отдельном случае стоит отталкиваться от целевой задачи и доменной специфики текстов.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5f1d4a",
   "metadata": {},
   "source": [
    "## TF-IDF `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e29b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_documents x Vocabular_size: (12, 10000)\n",
      "['abate' 'abhor' 'abhorr' 'abhorred' 'abide']\n",
      "[[0.         0.00361604 0.00259977 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "with open(settings.SOURCE_PATH + \"datasets/tiny_shakespear.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# разобьём текст на части - шаг необязательный, \n",
    "# просто искусственно делаем из 1 документа 10 разных)\n",
    "step = 100000\n",
    "docs = [text[i:i+step] for i in range(0, len(text), step)]\n",
    "\n",
    "tf_idf = TfidfVectorizer(\n",
    "    stop_words=stop_words, # сразу передадим стоп-слова\n",
    "    max_features=10_000, # размер словаря\n",
    "    ngram_range=(1,2), # 1-2 словесные n-gramы\n",
    "    min_df=2, # токен не реже, чем в 2 документах\n",
    "    max_df=0.95 # не учитываем токен с встречаемостью > 95%\n",
    "    ) \n",
    "# используем комбинацию методов fit и transform\n",
    "tf_idf_matrix = tf_idf.fit_transform(docs)\n",
    "\n",
    "print(\"N_documents x Vocabular_size:\", tf_idf_matrix.shape)\n",
    "print(tf_idf.get_feature_names_out()[:5]) # 5 токенов из словаря\n",
    "print(tf_idf_matrix[0, :5].todense()) # веса этих 5 токенов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfff3dc",
   "metadata": {},
   "source": [
    "Метод автоматически токенизирует тексты, используя регулярное выражение по умолчанию, однако можно передать свой токенизатор в аргумент `tokenizer`.\n",
    "`TfidfVectorizer` также содержит несколько интересных параметров:\n",
    "\n",
    "- `max_features` — позволяет заранее установить размер словаря — отбирает топ самых частотных токенов при превышении max_features. Можно использовать, чтобы повысить производительность: меньше матрица → быстрее считаем.\n",
    "  \n",
    "- `max_df, min_df` — числовое ограничение. Могут быть целым числом 2, 3, 4 или долей в диапазоне [0, 1.0]. Помогают учесть частотность встречаемости слов в документах — можно удалять как слишком частые слова в корпусе (специфичные), так и слишком редкие.\n",
    "    \n",
    "- `ngram_range` — опция настройки размера n-gram.\n",
    "    \n",
    "- `analyzer` — позволяет применять расчёт n_gram посимвольно (char) или по словам (word)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39138f",
   "metadata": {},
   "source": [
    "Часто встречается один и тот же вид слова, это указывает на одну из слабостей использования метода tf-idf напрямую — невозможность работы со словоформами \n",
    ">***Словоформа — это вариант слова, принимающий различные грамматические формы в зависимости от его использования в предложении, включая изменения по временам, числам, падежам и т. д. Например, для слова «ягнёнок» есть такие словоформы: «ягнёнка», «ягнёнку», «ягнята», «ягнятами» и др.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf36ceb",
   "metadata": {},
   "source": [
    "TF-IDF для таких задач:\n",
    "\n",
    "- Классификация текстов — подача в модели: градиентный бустинг, логистическую регрессию и другое.\n",
    "\n",
    "- Поиск похожих документов — анализ близости векторов.\n",
    "    \n",
    "- Выделение ключевых слов — оценка значимости токенов на основе частотности.\n",
    "   \n",
    "- Тематическое моделирование — выявление скрытых тем в документах."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-experiments-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
