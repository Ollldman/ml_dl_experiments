{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51b571d5",
   "metadata": {},
   "source": [
    "# Библиотекид ля работы с CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e538582",
   "metadata": {},
   "source": [
    "## Предобработка изображений с помощью cv2 и Pillow\n",
    "\n",
    "Две самые популярные библиотеки для работы с приложениями — это `cv2` и `Pillow`.\n",
    "\n",
    "- cv2. Производительная библиотека, написанная на C++, которая позволяет осуществлять множество операций над изображениями и видео. Она содержит много методов для их обработки, которые были популярны до появления нейросетей, то есть в эпоху так называемого классического Computer Vision.\n",
    "\n",
    "- Pillow. Более молодая библиотека, написанная на языке C. Её основной фокус направлен на предобработку изображений и простоту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd22fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from ml_dl_experiments import settings\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jigsawpieces/dog-api-images/main/eskimo/n02109961_10021.jpg\"\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "image_path: str = settings.SOURCE_PATH + \"ml_dl/CNN/eskimo_dog.jpg\"\n",
    "\n",
    "with open(image_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "img_pil = Image.open(image_path)\n",
    "img_cv2 = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e51b584",
   "metadata": {},
   "source": [
    "В отличие от Pillow, cv2 использует другой порядок каналов при загрузке — BGR, а не RGB. Поэтому при чтении необходимо приводить изображение в стандартный формат с помощью перестановки каналов cv2.COLOR_BGR2RGB в методе cv2.cvtColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d70a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB) # type:ignore BGR -> RGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35e15486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# не выведет ничего при ~равенстве массивов, иначе - ошибка\n",
    "np.testing.assert_allclose(img_cv2, img_pil, atol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae9a02",
   "metadata": {},
   "source": [
    "Выполним масштабирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8667b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nNot equal to tolerance rtol=1e-07, atol=1e-08\n\nMismatched elements: 94421 / 150528 (62.7%)\nMax absolute difference among violations: 56\nMax relative difference among violations: 3.\n ACTUAL: array([[[ 91, 116,  58],\n        [ 88, 113,  56],\n        [ 89, 114,  57],...\n DESIRED: array([[[ 90, 115,  58],\n        [ 88, 113,  56],\n        [ 89, 114,  57],...",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m img_pil = img_pil.resize(shape)\n\u001b[32m      3\u001b[39m img_cv2 = cv2.resize(img_cv2, shape)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtesting\u001b[49m\u001b[43m.\u001b[49m\u001b[43massert_allclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_cv2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_pil\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m \n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-dl-experiments-m4qsJGWw-py3.13/lib/python3.13/site-packages/numpy/testing/_private/utils.py:921\u001b[39m, in \u001b[36massert_array_compare\u001b[39m\u001b[34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf, strict, names)\u001b[39m\n\u001b[32m    916\u001b[39m         err_msg += \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m + \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join(remarks)\n\u001b[32m    917\u001b[39m         msg = build_err_msg([ox, oy], err_msg,\n\u001b[32m    918\u001b[39m                             verbose=verbose, header=header,\n\u001b[32m    919\u001b[39m                             names=names,\n\u001b[32m    920\u001b[39m                             precision=precision)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(msg)\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtraceback\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: \nNot equal to tolerance rtol=1e-07, atol=1e-08\n\nMismatched elements: 94421 / 150528 (62.7%)\nMax absolute difference among violations: 56\nMax relative difference among violations: 3.\n ACTUAL: array([[[ 91, 116,  58],\n        [ 88, 113,  56],\n        [ 89, 114,  57],...\n DESIRED: array([[[ 90, 115,  58],\n        [ 88, 113,  56],\n        [ 89, 114,  57],..."
     ]
    }
   ],
   "source": [
    "shape = (224, 224)\n",
    "img_pil = img_pil.resize(shape)\n",
    "img_cv2 = cv2.resize(img_cv2, shape)\n",
    "\n",
    "np.testing.assert_allclose(img_cv2, img_pil, atol=1e-8) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285d522",
   "metadata": {},
   "source": [
    "Здесь уже видны различия — из-за разных алгоритмов масштабирования и их реализации мы не всегда можем получить одинаковые результаты, вызывая в общем схожие методы. Расхождения обычно небольшие, так что явных артефактов на изображении не будет. Но лучше всё-таки помнить про это, фиксировать один алгоритм ремасштабирования и в дальнейшем придерживаться его."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b1f26",
   "metadata": {},
   "source": [
    "Нормализация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08f4a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 0.9490196078431372 0.9529411764705882\n"
     ]
    }
   ],
   "source": [
    "img_pil = np.array(img_pil) / 255\n",
    "img_cv2 = img_cv2 / 255\n",
    "\n",
    "print(np.min(img_cv2), np.min(img_pil), np.max(img_cv2), np.max(img_pil)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a03801eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_pil.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a79836",
   "metadata": {},
   "source": [
    "Для изображения с shape=(224, 224, 3) в формате RGB, чтобы вычислить среднее и стандартное отклонение для каждого канала с помощью numpy, используем следующие подходы:\n",
    "\n",
    "## Расчёт mean и std для каналов\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Пусть img — numpy-массив изображения с shape=(224, 224, 3)\n",
    "mean = np.mean(img, axis=(0, 1))  # Среднее для каждого канала\n",
    "std = np.std(img, axis=(0, 1))    # Стандартное отклонение для каждого канала\n",
    "```\n",
    "\n",
    "## Детали\n",
    "\n",
    "- Аргумент axis=(0, 1) означает, что функция считается по высоте и ширине, отдельно для каждого канала (последнее измерение — RGB).\n",
    "- В результате mean и std будут массивами из 3 элементов: [mean_R, mean_G, mean_B] и [std_R, std_G, std_B].\n",
    "\n",
    "## Пример вывода\n",
    "\n",
    "```python\n",
    "print(\"Mean (R, G, B):\", mean)\n",
    "print(\"Std  (R, G, B):\", std)\n",
    "```\n",
    "\n",
    "Для типичных задач машинного обучения mean и std используют для нормализации входных данных изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31a177",
   "metadata": {},
   "source": [
    "При загрузке порядок размерностей H × W × C различается от того, какой требует PyTorch при использовании моделей — C × H × W.\n",
    "Чтобы преобразовать изображения к формату Channels × Heigth × Width, можно воспользоваться стандартной перестановкой каналов из NumPy: методом transpose. Он применяется к массивам и принимает на вход индексы каналов в нужном нам порядке. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96012e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры до: (356, 356, 3)\n",
      "Размеры после: (3, 356, 356)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размеры до: {img_cv2.shape}\")\n",
    "\n",
    "img_cv2 = img_cv2.transpose(2, 0, 1)\n",
    "print(f\"Размеры после: {img_cv2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8016f052",
   "metadata": {},
   "source": [
    "Библиотека cv2 может быть предпочтительна в ситуациях, когда нужна более быстрая обработка.\n",
    "cv2, написанная на C++, быстрее, чем Pillow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29c5ad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.4 , 0.42, 0.44]), array([0.2 , 0.21, 0.22]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# читаем картинку\n",
    "img_cv2 = cv2.imread(image_path)\n",
    "# переводим в RGB цвета\n",
    "img_cv2 = cv2.cvtColor(img_cv2, cv2.COLOR_BGR2RGB)\n",
    "# Масшатбируем\n",
    "shape = (356, 356)\n",
    "img_cv2 = cv2.resize(img_cv2, shape)\n",
    "# Приводим к диапазону 0-1\n",
    "img_cv2 = img_cv2 / 255\n",
    "# Поканальная нормализация\n",
    "# mean = np.mean(img_cv2, axis=(0, 1))\n",
    "# std = np.std(img_cv2, axis=(0, 1))\n",
    "\n",
    "mean = np.array([0.4, 0.42, 0.44])\n",
    "std =np.array([0.2, 0.21, 0.22])\n",
    "\n",
    "img_cv2 = (img_cv2 - mean) / std \n",
    "\n",
    "print(img_cv2[0,0,1].round(3))\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e7003f",
   "metadata": {},
   "source": [
    "Чаще всего представленные библиотеки для задач DL используются как загрузчики данных — выбор конкретного фреймворка в таком сценарии использования не так критичен. \n",
    "\n",
    "Но когда вы хотите выполнять обработку видео, применять какие-то методы из классического компьютерного зрения и при этом иметь большую скорость обработки, стоит отдать предпочтение cv2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c888138b",
   "metadata": {},
   "source": [
    "## Библиотека  timm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06f5c4",
   "metadata": {},
   "source": [
    "Библиотека timm — Torch IMage Models, входящая в экосистему платформы huggingface. timm значительно упрощает загрузку, использование и настройку моделей под конкретные задачи CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7022c97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее число моделей: 1279\n",
      "Первые 5 моделей семейства resnet: \n",
      "['resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26']\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "total_models = len(timm.list_models())\n",
    "print(f\"Общее число моделей: {total_models}\")\n",
    "print(f\"Первые 5 моделей семейства resnet: \\n{timm.list_models(filter='resnet*')[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7a990",
   "metadata": {},
   "source": [
    "Нобходимую модель нужно скачать и проинициализировать. Делается это с помощью метода `create_model(model_name=..)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c05bc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (drop_block): Identity()\n",
      "      (act1): ReLU(inplace=True)\n",
      "      (aa): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model(model_name=\"resnet18\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b508096",
   "metadata": {},
   "source": [
    "По умолчанию модели инициализируются случайными весами. Чтобы использовать предобученный вариант, укажем pretrained=True, а также визуализируем конфигурационный файл с помощью атрибута модели pretrained_cfg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5bca0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3483cecc50144f8fbc6d1ea549961361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'url': 'https://github.com/huggingface/pytorch-image-models/releases/download/v0.1-rsb-weights/resnet18_a1_0-d63eafa0.pth',\n",
       " 'hf_hub_id': 'timm/resnet18.a1_in1k',\n",
       " 'architecture': 'resnet18',\n",
       " 'tag': 'a1_in1k',\n",
       " 'custom_load': False,\n",
       " 'input_size': (3, 224, 224),\n",
       " 'test_input_size': (3, 288, 288),\n",
       " 'fixed_input_size': False,\n",
       " 'interpolation': 'bicubic',\n",
       " 'crop_pct': 0.95,\n",
       " 'test_crop_pct': 1.0,\n",
       " 'crop_mode': 'center',\n",
       " 'mean': (0.485, 0.456, 0.406),\n",
       " 'std': (0.229, 0.224, 0.225),\n",
       " 'num_classes': 1000,\n",
       " 'pool_size': (7, 7),\n",
       " 'first_conv': 'conv1',\n",
       " 'classifier': 'fc',\n",
       " 'license': 'apache-2.0',\n",
       " 'origin_url': 'https://github.com/huggingface/pytorch-image-models',\n",
       " 'paper_ids': 'arXiv:2110.00476'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(\n",
    "    model_name=\"resnet18\",\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "model.pretrained_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f40d53",
   "metadata": {},
   "source": [
    "Основные параметры, которые могут быть полезны:\n",
    "\n",
    "- `tag` — как правило, содержит информацию об обучении модели. В частности, здесь a1_in1k — означает, что модель обучалась на датасете ImageNet с 1000 классами.\n",
    "    \n",
    "- `input_size` — размер изображений, использованный при тренировке.\n",
    "    \n",
    "- `fixed_input_size` — атрибут, указывающий на жёсткость ограничения входного размера изображений. В данном случае False означает, что ограничений нет.\n",
    "    \n",
    "- `mean и std` — коэффициенты для нормализации изображений, использованные при тренировке."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66f7074",
   "metadata": {},
   "source": [
    "Для загрузки pretrained модели с помощью timm из локального файла нужно сделать следующее:\n",
    "\n",
    "## Загрузка модели timm с локальной pretrained весовой:\n",
    "\n",
    "1. Создайте модель с `pretrained=False`, чтобы не загружать веса из интернета.\n",
    "2. Загрузите веса из локального файла (обычно `.pth` или `.bin`) с помощью `torch.load`.\n",
    "3. Подгрузите веса в модель через `model.load_state_dict`.\n",
    "4. Модель готова к использованию.\n",
    "\n",
    "## Пример кода:\n",
    "\n",
    "```python\n",
    "import timm\n",
    "import torch\n",
    "\n",
    "# Создаем модель без загрузки pretrained весов из интернета\n",
    "model = timm.create_model('имя_модели', pretrained=False)\n",
    "\n",
    "# Путь к файлу с весами на локальном диске\n",
    "weights_path = 'путь/до/файла/model_weights.pth'\n",
    "\n",
    "# Загружаем веса\n",
    "state_dict = torch.load(weights_path, map_location='cpu')\n",
    "\n",
    "# Если в state_dict есть ключ 'model' или 'state_dict', нужно извлечь\n",
    "# Пример для common case:\n",
    "if 'state_dict' in state_dict:\n",
    "    state_dict = state_dict['state_dict']\n",
    "\n",
    "# Загружаем веса в модель\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Модель готова для использования\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "### Особенности\n",
    "- Убедитесь, что название модели в create_model совпадает с той, для которой предназначены веса.\n",
    "- Если файл весов был сохранен с использованием `DataParallel` (с префиксом 'module.' в ключах), возможно потребуется подправить ключи перед загрузкой.\n",
    "\n",
    "Так вы можете передать путь к файлу с pretrained весами и загрузить их локально без подключения к интернету."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a3c756",
   "metadata": {},
   "source": [
    "Оценим инференс одной модели. maxvit_large_tf_384\n",
    "1. Загрузим картинку и модель\n",
    "2. Масштабируем в нужный формат\n",
    "3. Нормализуем в диапазон от 0 до 1, затем в формат, необходимый для тренировки конкретной модели\n",
    "4. Вывести результирующую размерность изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8baebf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ml_dl_experiments import settings\n",
    "# Загрузим изображение\n",
    "image_path: str = settings.SOURCE_PATH + \"ml_dl/CNN/eskimo_dog.jpg\"\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img = img / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be7d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6124f73ad2b423ba830a65ce241514f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/849M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'url': '',\n",
       " 'hf_hub_id': 'timm/maxvit_large_tf_384.in1k',\n",
       " 'architecture': 'maxvit_large_tf_384',\n",
       " 'tag': 'in1k',\n",
       " 'custom_load': False,\n",
       " 'input_size': (3, 384, 384),\n",
       " 'fixed_input_size': True,\n",
       " 'interpolation': 'bicubic',\n",
       " 'crop_pct': 1.0,\n",
       " 'crop_mode': 'squash',\n",
       " 'mean': (0.5, 0.5, 0.5),\n",
       " 'std': (0.5, 0.5, 0.5),\n",
       " 'num_classes': 1000,\n",
       " 'pool_size': (12, 12),\n",
       " 'first_conv': 'stem.conv1',\n",
       " 'classifier': 'head.fc',\n",
       " 'license': 'apache-2.0'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузим модель, выведем конфиги\n",
    "\n",
    "model = timm.create_model(\n",
    "    model_name=\"maxvit_large_tf_384\",\n",
    "    pretrained=True\n",
    ")\n",
    "\n",
    "model.pretrained_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0431e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = np.array(model.pretrained_cfg['mean']), np.array(model.pretrained_cfg['std'])\n",
    "shape = (384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6e19ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 384, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Изменим масштаб и нормализуем под модель\n",
    "img = cv2.resize(img, shape)\n",
    "\n",
    "img = (img - mean) / std\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a688a80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[65.1152, 23.5729,  0.7286,  0.1099,  0.1031]]),\n",
       " tensor([[250, 248, 249, 174, 270]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import ToTensor\n",
    "tensor = ToTensor()\n",
    "image_tensor = tensor(img)\n",
    "image_tensor = image_tensor.to(torch.float32)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(image_tensor.unsqueeze(0))\n",
    "\n",
    "result = out.softmax(dim=1)\n",
    "top5_probabilities, top5_class_indices = torch.topk(result * 100, k=5)\n",
    "top5_probabilities, top5_class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac69b8c6",
   "metadata": {},
   "source": [
    "Ещё одна важная часть возможностей библиотеки связана с инициализацией самой модели.\n",
    "\n",
    "Большая часть моделей обучались на задачу классификации и построены по упрощённому принципу:\n",
    "\n",
    "- последовательные слои конволюций,\n",
    "    \n",
    "- агрегирование значений карт признаков,\n",
    "\n",
    "- линейный классификатор.\n",
    "\n",
    "Для конечной задачи не всегда интересен результирующий классификатор — больше наиболее важны признаки модели (тензора до линейного слоя). \n",
    "\n",
    "Производить `feature-extraction` в `timm` легко — достаточно указать `features_only=True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e2f82e",
   "metadata": {},
   "source": [
    "```py\n",
    "model = timm.create_model(\"efficientnet_b0\",\n",
    "                          pretrained=True,\n",
    "                          features_only=True)\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out = model(x)\n",
    "print([x.shape for x in out])\n",
    "```\n",
    "\n",
    "```py\n",
    "[torch.Size([1, 16, 112, 112]), \n",
    " torch.Size([1, 24, 56, 56]), \n",
    " torch.Size([1, 40, 28, 28]), \n",
    " torch.Size([1, 112, 14, 14]), \n",
    " torch.Size([1, 320, 7, 7])]\n",
    "```\n",
    "\n",
    "Получаем разноуровневые признаки из модели — какие-то будут акцентироваться на крупных деталях, какие-то на мелких. Часто использование признаков разных уровней помогает в решении задачи или улучшении качества существующей.\n",
    "\n",
    "Например, это эффективно в задаче детекции, когда мы хотим обнаруживать как локальные паттерны (ранние слои: цвета и градиенты), так и глобальные (части объектов: глаза, руки и т. д.). \n",
    "\n",
    "Под «уровнем» здесь подразумевается один из последовательных блоков. В частности, в `efficientnet_b0` это индекс группы `InvertedResidual` - блоков — таких в этой модели 5 штук."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb579e56",
   "metadata": {},
   "source": [
    "Количеством извлекаемых признаков можно управлять с помощью параметра `out_indices`, в который передаётся список индексов, соответствующий блокам признаков. \n",
    "\n",
    "Для некоторых задач могут быть релевантны только признаки с самых глубоких слоёв — и для модели из примера выше мы могли бы извлечь, например, только 2 последних признака. \n",
    "\n",
    "Чтобы понять общее количество возвращаемых признаков для конкретной модели, можно сначала указать только `features_only`, а по количеству тензоров понять, какие индексы можно указывать.\n",
    "\n",
    "```py\n",
    "model = timm.create_model(\"efficientnet_b0\",\n",
    "                          pretrained=True,\n",
    "                          features_only=True,\n",
    "                          out_indices=[3, 4])\n",
    "\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "out = model(x)\n",
    "print([x.shape for x in out])\n",
    "\n",
    "[torch.Size([1, 112, 14, 14]), \n",
    " torch.Size([1, 320, 7, 7])]\n",
    "```\n",
    "\n",
    "Без библиотеки timm нам пришлось бы вручную реализовывать поддержку каждой архитектуры на PyTorch, а timm берёт эту работу на себя и обеспечивает единый интерфейс для любых моделей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-experiments-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
