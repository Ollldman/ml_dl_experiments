import numpy as np
from numpy.typing import ArrayLike


def gelu(x: ArrayLike) -> np.ndarray:
    """
    Это функция активации, основанная на гауссовом интеграле ошибок (erf). 
    Формула
    GELU(x)=x⋅Φ(x), где Φ(x)=∫(from -inf to x) (1 / sqrt(2π))⋅pow(e,(-t^2)/2)dt.

    В приближенном виде часто используют:
    GELU(x) = 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi)*(x + 0.044715 * x**3)))

    GELU плавно подавляет небольшие отрицательные значения, поэтому модель стабильнее сходится и лучше обобщает результаты. Эта функция эффективна, но из-за повышенной вычислительной сложности её используют редко. 

    Плюсы:

    Плавность и непрерывность первых двух производных — полезно для оптимизаторов.
    Отрицательные значения подавляются мягко, поэтому уменьшается риск «мёртвых» нейронов.
    Функция часто улучшает качество моделей в NLP — например, в трансформерах.

    Минусы:

    Вычислительно дороже, чем ReLU и LeakyReLU.
    Требует либо интеграла (через erf), либо приближения (tanh) — чуть сложнее в реализации.
    """
    x = np.asarray(x)
    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi)*(x + 0.044715 * x**3)))