import numpy as np

def tanh(x: float) -> float:
    """
    tanh(x) = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

    Эта функция активации может использоваться в скрытых слоях нейросетей. 
    
    Плюсы:

    Выходы центрированы вокруг нуля — ускоряет сходимость, то есть быстрее обучается.
    В центральной области градиенты более крутые, чем у сигмоиды. Это смягчает проблему затухающих градиентов.

    Минусы:

    Остаётся проблема затухающих градиентов. При больших по модулю значениях x градиент затухает, из-за этого глубокие сети медленно обучаются.
    
    Вычислительно затратнее сигмоиды.
    """
    return np.tanh(x)