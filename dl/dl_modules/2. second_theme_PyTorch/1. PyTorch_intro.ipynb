{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59c47fd",
   "metadata": {},
   "source": [
    "# Что такое PyTorch\n",
    "\n",
    "PyTorch — это фреймворк машинного обучения для языка Python с открытым исходным кодом, созданный на базе библиотеки Torch.\n",
    "\n",
    "Он позволяет очень быстро и гибко описывать нейронные сети: вместо множества строк матричных умножений и ручных вычислений градиентов мы будем работать с понятными и интуитивными абстракциями.\n",
    "\n",
    "С его помощью можно с минимальными усилиями превратить ранее написанный вручную код обучения в лаконичные и надёжные конструкции: от создания собственных датасетов до настройки функций активации и выбор оптимизатора. Это поможет вам быстрее прототипировать идеи, сосредоточиться на дизайне моделей и легко масштабировать их в дальнейшем.\n",
    "\n",
    "PyTorch быстро стал популярным благодаря удобству разработки и гибкости. В отличие от фреймворков со статическим графом (например, TensorFlow), PyTorch использует динамический граф вычислений: граф создаётся прямо во время выполнения кода. Это делает его более гибким и упрощает отладку: вы можете изменять архитектуру сети во время работы и пользоваться привычными средствами Python (print, отладчик).\n",
    "\n",
    "Код на PyTorch выглядит естественно для Python-разработчика и очень похож на NumPy-нотации. Кроме того, PyTorch оптимизирован под GPU: он эффективно использует видеокарты для обучения крупных моделей. Фреймворк обладает большой стандартной библиотекой слоёв, функций потерь и оптимизаторов, а также обширным сообществом исследователей и разработчиков, которые создают новые модели и обучающие материалы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a7d23",
   "metadata": {},
   "source": [
    "# torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a28b1",
   "metadata": {},
   "source": [
    "В основе PyTorch лежит объект тензор — многомерный массив чисел, аналогичный numpy.ndarray, но с дополнительными возможностями.\n",
    "\n",
    "Тензоры могут храниться на CPU или GPU и, при необходимости, отслеживать вычисленные градиенты.\n",
    "\n",
    "Интерфейс PyTorch специально сделан похожим на NumPy, чтобы разработчику было удобно работать с данными.\n",
    "\n",
    "Давайте посмотрим на несколько тензоров в действии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dcf38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) torch.float32\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]]) torch.int64\n",
      "tensor([[0.7798, 0.4136, 0.0503],\n",
      "        [0.2354, 0.7947, 0.9279],\n",
      "        [0.0162, 0.9562, 0.2846]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0])              # 1D-тензор из списка\n",
    "b = torch.zeros((2,3), dtype=torch.int64)      # матрица 2х3 из нулей(64-битные целы)\n",
    "c = torch.rand(3, 3)                           # случайный тензор 3х3, равномерное распределение\n",
    "\n",
    "print(a, a.dtype)\n",
    "print(b, b.dtype)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9fa37",
   "metadata": {},
   "source": [
    "Тензоры поддерживают стандартные арифметические операции (сложение, умножение, матричные произведения и другие) и функциональные преобразования. Запустите код, чтобы проверить, как работают арифметические операции в PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80491943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.7183, -1.8647, 23.0855])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.tensor([1.0, -2.0, 3.0])\n",
    "y = torch.exp(x)    # элемент-wise экспонента\n",
    "z = x + y           # сложение поэлементно\n",
    "print(z) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0be57",
   "metadata": {},
   "source": [
    "При необходимости тензоры можно передавать на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fcf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    a = a.to('cuda')  # переместить на CUDA-устройство "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4b370",
   "metadata": {},
   "source": [
    "Поскольку PyTorch тесно интегрирован с NumPy, тензоры можно конвертировать в numpy.ndarray и обратно — например, через torch.from_numpy или метод tensor.numpy()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f82d13",
   "metadata": {},
   "source": [
    "# Автоматическое дифференцирование (Autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39ecc3",
   "metadata": {},
   "source": [
    "PyTorch имеет встроенную систему автоматического дифференцирования (autograd), которая вычисляет градиенты. Если у тензора установлено свойство requires_grad=True, PyTorch запоминает все операции над ним и может потом вычислить градиенты по цепному правилу. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf192e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)  # создаём тензор, указывая, что нужны градиенты\n",
    "y = x * 2\n",
    "\n",
    "z = y.mean()      # свёртка: усредняем все элементы\n",
    "z.backward()      # выполняем обратный проход (backpropagation)\n",
    "print(x.grad)     # смотрим градиент dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde289e6",
   "metadata": {},
   "source": [
    "Вот так просто мы посчитали градиент нашего тензора!\n",
    "\n",
    "В этом примере z = (x*2).mean() = 2.0, и после z.backward() в x.grad окажутся градиенты dz/dx. Автоматическая дифференциация позволяет легко реализовать обратный проход при обучении нейросетей. Градиенты накапливаются в атрибуте .grad тензора и могут быть использованы оптимизатором для обновления параметров.\n",
    "\n",
    "Вы увидели, как автоматическое дифференцирование тщательно отслеживает операции с тензорами и накапливает градиенты. \n",
    "\n",
    "Следующий шаг — организовать сами вычисления в удобные блоки. Вместо того чтобы вручную писать последовательность матричных умножений и вызов backward(), мы хотим упаковать слои сети в единый объект с встроенным хранением параметров и логикой прямого прохода. Для этого и служит класс nn.Module, который упрощает создание, хранение и обучение любой архитектуры нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba255d",
   "metadata": {},
   "source": [
    "# Модели и nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d774e",
   "metadata": {},
   "source": [
    "В PyTorch нейронные сети строятся из модулей, основанных на базовом классе torch.nn.Module. Модуль nn.Module — это любое преобразование данных с параметрами (например, слой сети). Все стандартные слои (линейный слой, свёрточный слой, активации и т. д.) — это подклассы nn.Module. Саму нейросеть принято реализовывать как подкласс nn.Module, где в методе __init__ задаются слои, а в forward описывается логика прямого прохода.\n",
    "\n",
    "Подкласс nn.Module позволяет определить сразу две ключевые части любой нейросети:\n",
    "-  Инициализацию, где мы создаём слои и другие компоненты.\n",
    "-  Логику прямого прохода.\n",
    "\n",
    "Рассмотрим каждый метод по отдельности.\n",
    "Метод __init__ отвечает за инициализацию параметров и создание слоёв/функций активации. Когда мы наследуемся от nn.Module, первое, что нужно сделать внутри __init__, — это вызвать конструктор базового класса, чтобы внутри него успели инициализироваться внутренние механизмы PyTorch: регистрация параметров, хранение списка дочерних модулей и т. п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157b1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Важно: сначала вызываем конструктор nn.Module\n",
    "        # Создаём линейный слой: принимает 10 входов, выдаёт 5 выходов\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=5) \n",
    "        self.relu = nn.ReLU()         # ReLU-активация "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc7fed",
   "metadata": {},
   "source": [
    "super().__init__() запускает конструктор класса nn.Module. В результате внутри экземпляра появится структура, позволяющая хранить все слои, параметры (веса, смещения) и вложенные модули.\n",
    "\n",
    "Без вызова super().__init__() PyTorch не увидит созданные в дальнейшем слои и не сможет корректно собрать список параметров для оптимизатора.\n",
    "\n",
    "После инициализации базового класса можно добавить в наш класс любой набор слоёв и функций активации. Все элементы, присвоенные атрибутам экземпляра, которые сами являются подклассами nn.Module (например, nn.ReLU и т. д.), автоматически будут зарегистрированы как дочерние модули.\n",
    "\n",
    "nn.Linear(in_features, out_features) создаёт слой, который хранит матрицу весов размера (in_features, out_features) и вектор смещений длины out_features. \n",
    "\n",
    "При создании по умолчанию веса и смещения инициализируются случайно — например, из нормального распределения, сконструированного отдельными правилами.\n",
    "\n",
    "Параметры weights и bias, с которыми мы сейчас познакомимся, автоматически получают атрибут requires_grad=True, то есть по ним будут рассчитываться градиенты при расчёте обратного распространения ошибки — loss.backward().\n",
    "\n",
    "nn.ReLU() — это слой, который не содержит обучаемых параметров, а лишь преобразует входной тензор по правилу f(z) = max(0, z). Поскольку nn.ReLU() тоже наследуется от nn.Module, PyTorch знает, что этот модуль — часть графа, и сможет правильно вставить операцию ReLU в вычислительный граф."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85facd",
   "metadata": {},
   "source": [
    "Когда в __init__ мы объявили все слои и функции активации, метод forward отвечает за логику прямого прохода: он получает на вход батч данных, прокатывает его через все слои, применяя необходимые преобразования, и возвращает выход. Стандартная сигнатура выглядит так:\n",
    "\n",
    "```python\n",
    "\n",
    "def forward(self, x):\n",
    "    # x - входной тензор формы [batch_size, in_features]\n",
    "    # Здесь мы пишем последовательность операций вида:\n",
    "    # x = self.fc1(x)\n",
    "    # x = self.relu(x)\n",
    "    # x = self.fc2(x)\n",
    "    # return x\n",
    "\n",
    "```\n",
    "\n",
    "Параметр x  должен быть тензором с той размерностью, которую ожидает первый слой. Метод forward неявно вызывается при том, что мы делаем model(input_tensor). Под капотом PyTorch перенаправляет этот вызов в forward.\n",
    "\n",
    "<div background-color=green>\n",
    "Абсолютно любой код внутри forward будет использован при построении динамического графа. Можно писать условные операторы, циклы, вызывать другие функции и т. д. Всё, что приводит к операции над тензорами с requires_grad=True, станет частью графа, по которому потом будет проходить backward().\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be88e3",
   "metadata": {},
   "source": [
    "Как работает nn.Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0330ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Весовой тензор W: torch.Size([5, 10])\n",
      "Вектор смещений b: torch.Size([5])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "model = nn.Linear(in_features=10, out_features=5)\n",
    "print(\"Весовой тензор W:\", model.weight.shape)   # torch.Size([5, 10])\n",
    "print(\"Вектор смещений b:\", model.bias.shape)    # torch.Size([5])\n",
    "\n",
    "\n",
    "# Проверим, что по умолчанию требуется вычисление градиентов\n",
    "print(model.weight.requires_grad)\n",
    "print(model.bias.requires_grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8f85d",
   "metadata": {},
   "source": [
    "Проверим работу RelU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9011610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 2.],\n",
      "        [3., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([[-1.0, 0.0, 2.0], [3.0, -5.0, 1.0]])\n",
    "y = relu(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db7d4b",
   "metadata": {},
   "source": [
    "Пошаговый пример\n",
    "Допустим, есть два линейных слоя и одна активация ReLU. Логика такая:\n",
    "\n",
    "1. На вход в forward приходит тензор x формы [batch_size, 10].\n",
    "2. Сначала мы применяем линейное преобразование — есть матрица весов (10 → 5), получается тензор формы [batch_size, 5].\n",
    "3. К полученному результату применяем ReLU.\n",
    "4. Результат ReLU снова пропускаем через линейный слой (5 → 1), получаем [batch_size, 1].\n",
    "5. Метод возвращает этот выходной тензор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976762a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7281,  0.9982,  0.7531,  0.8091,  2.0482, -0.1595, -0.8416, -0.4780,\n",
      "          1.0232, -0.0787],\n",
      "        [-0.9667,  0.2407, -0.3744,  0.7973,  1.7687, -0.8267,  0.1825,  0.2015,\n",
      "          1.5891, -0.1798],\n",
      "        [-0.0836, -0.7374, -1.4650,  1.0841,  0.0646, -0.6541,  0.8034, -0.9726,\n",
      "          0.1947, -1.0811]])\n",
      "tensor([[0.3407],\n",
      "        [0.4364],\n",
      "        [0.5301]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Предположим, что мы уже создали в __init__ следующие слои:\n",
    "fc1 = nn.Linear(10, 5)     # из 10 входов в 5 нейронов\n",
    "relu = nn.ReLU()\n",
    "fc2 = nn.Linear(5, 1)      # из 5 входов в 1 нейрон\n",
    "\n",
    "\n",
    "# А вот упрощённая функция forward, написанная вне класса:\n",
    "def forward_pass(x):\n",
    "    x1 = fc1(x)            # применили первый линейный слой\n",
    "    x2 = relu(x1)          # применили ReLU\n",
    "    x3 = fc2(x2)           # применили второй линейный слой\n",
    "    return x3\n",
    "\n",
    "\n",
    "# Создадим тестовый входной батч: batch_size=3, in_features=10\n",
    "\n",
    "input_tensor = torch.randn(3, 10)\n",
    "output_tensor = forward_pass(input_tensor)\n",
    "print(input_tensor)\n",
    "print(output_tensor)  # tensor of shape [3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8ecfc",
   "metadata": {},
   "source": [
    "Видно, что мы неявно строим динамический граф: когда выполняются `fc1(x)`, `relu(x1)` и `fc2(x2)`, PyTorch запоминает, какие операции были применены и к каким входным тензорам. \n",
    "\n",
    "В дальнейшем, если мы сформируем функцию потерь от `output_tensor` и вызовем `loss.backward()`, будут вычислены градиенты по всем параметрам `fc1.weight`, `fc1.bias`, `fc2.weight`, `fc2.bias`, вы можете самостоятельно добавить в код-сниппет выводы этих параметров и посмотреть, как они будут выглядеть.\n",
    "\n",
    "**Что должен ожидать метод forward на вход**:\n",
    "\n",
    "- Обычно это тензор формы (batch_size, in_features) — если вы работаете с табличными данными или одним линейным слоем.\n",
    "\n",
    "- В случае с изображениями это может быть тензор (batch_size, channels, height, width).\n",
    "\n",
    "- Главное, чтобы форма тензора совпадала с формой, на которую настроен первый слой. Например, если первый слой — nn.Linear(10, 5), то входные данные должны иметь вторую размерность 10.\n",
    "\n",
    "**Что должен отдавать метод forward на выход**:\n",
    "\n",
    "- Как правило, это тензор (batch_size, out_features), где out_features — число выходных нейронов в последнем слое.\n",
    "- Например, при регрессии на одно число мы получим (batch_size, 1), при классификации на 10 классов — (batch_size, 10) и т. д.\n",
    "- Внутри forward мы можем применять дополнительные нелинейности, нормализации, дропауты, но результатом всегда должен быть тензор, с которым затем мы сравним выход с метками и посчитаем лосс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69717c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n",
      "Всего параметров: 61\n"
     ]
    }
   ],
   "source": [
    "from SimpleNN import SimpleNN\n",
    "# Пример использования:\n",
    "model = SimpleNN()\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Можно сразу посмотреть, сколько параметров зарегистрировано:\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Всего параметров: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcd261",
   "metadata": {},
   "source": [
    "При вызове model(input_tensor) внутри автоматически сработает forward, и на выходе появятся предсказания.\n",
    "\n",
    "Все параметры (fc1.weight, fc1.bias, fc2.weight, fc2.bias) автоматически попадут в model.parameters(), поэтому мы сможем передать их в оптимизатор, например:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504f8e4",
   "metadata": {},
   "source": [
    "Если впоследствии мы вычислим лосс на основе предсказаний и метода loss.backward(), PyTorch аккуратно посчитает градиенты по всем весам и смещениям, а затем optimizer.step() обновит их.\n",
    "Таким образом, мы подробно разобрали, как внутри nn.Module организованы ключевые методы:\n",
    "\n",
    "    __init__ — для создания и регистрации слоёв,\n",
    "    forward — для описания потока данных.\n",
    "\n",
    "Вместе они образуют простую, но полную структуру нейросети, которую можно сразу обучать или расширять дополнительными слоями и функциональностью:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc4dbb",
   "metadata": {},
   "source": [
    "Теперь, когда наша модель определена и мы понимаем, как настроить прямой и обратный проход, возникает следующий важный вопрос: откуда будут браться данные и как они попадут внутрь нейронной сети? \n",
    "\n",
    "В PyTorch есть специальные инструменты — классы Dataset и DataLoader. Они позволяют:\n",
    "\n",
    "    Не писать вручную циклы по файлам или спискам.\n",
    "    Легко организовать разбиение на батчи, перемешивание и, при необходимости, загрузку данных на GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60728287",
   "metadata": {},
   "source": [
    "# Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a89b3e5",
   "metadata": {},
   "source": [
    "Для работы с данными PyTorch предоставляет два основных примитива: Dataset и DataLoader. \n",
    "\n",
    "Класс torch.utils.data.Dataset — это абстрактный класс, представляющий набор данных. \n",
    "\n",
    "Пользовательский датасет создаётся через наследование от Dataset и переопределение методов __len__ и __getitem__.\n",
    "\n",
    "Метод __len__ должен возвращать число образцов в датасете, а __getitem__ — один образец (обычно входные данные и метку) по индексу.\n",
    "\n",
    "Построим простой датасет для пар (data, target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4905342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (0, 0)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "# Создадим датасет из списков чисел\n",
    "dataset = MyDataset(data=list(range(10)), targets=[2*x for x in range(10)])\n",
    "print(len(dataset), dataset[0])  # 10, (0, 0)\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f7af6",
   "metadata": {},
   "source": [
    "Мы получили нужный нам датасет, но как лучше всего его передать модели?\n",
    "\n",
    "Класс torch.utils.data.DataLoader берёт Dataset и оборачивает его в итератор, который возвращает мини‑батчи данных.\n",
    "\n",
    "DataLoader позволяет автоматически формировать батчи указанного размера, перемешивать данные (shuffle=True) и загружать их параллельно с помощью нескольких потоков (num_workers).\n",
    "\n",
    "Давайте создадим свой DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a78ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 9]) tensor([ 4,  2, 18])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Создадим датасет из списков чисел\n",
    "dataset = MyDataset(data=list(range(10)), targets=[2*x for x in range(10)])\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=3, shuffle=True, num_workers=0)\n",
    "for batch_data, batch_target in dataloader:\n",
    "    print(batch_data, batch_target)\n",
    "    break  # посмотрим только первый батч"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b81f78",
   "metadata": {},
   "source": [
    "Разбермеся на практике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a65e2a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Батч 1:\n",
      "Inputs:  tensor([0, 9, 2, 4])\n",
      "Targets: tensor([ 0, 18,  4,  8])\n",
      "Батч 2:\n",
      "Inputs:  tensor([3, 6, 1, 7])\n",
      "Targets: tensor([ 6, 12,  2, 14])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# 1. Реализуйте класс ToyDataset\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        # Сохраните data и targets в атрибуты\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Верните длину датасета\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Верните пару (вход, метка), соответствующую индексу idx\n",
    "        return self.data[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "# 2. Подготовьте входные списки\n",
    "raw_data = list(range(10)) # data = [0, 1, 2, ..., 9]\n",
    "raw_targets = [2 * x for x in raw_data]  # targets = [0, 2, 4, ..., 18]\n",
    "\n",
    "\n",
    "# 3. Создайте объект ToyDataset\n",
    "\n",
    "dataset = ToyDataset(raw_data, raw_targets)\n",
    "\n",
    "\n",
    "# 4. Оберните датасет в DataLoader с batch_size=4 и shuffle=True\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "# Переберите первые два батча и выведите их на экран\n",
    "for batch_idx, (batch_inputs, batch_targets) in enumerate(dataloader):\n",
    "    print(f\"Батч {batch_idx + 1}:\")\n",
    "    print(\"Inputs: \", batch_inputs)\n",
    "    print(\"Targets:\", batch_targets)\n",
    "    if batch_idx == 1:  # остановимся после второго батча\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887a5eb2",
   "metadata": {},
   "source": [
    "# Функции активации и оптимизаторы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42dbad",
   "metadata": {},
   "source": [
    "В PyTorch уже реализовано множество функций активации и алгоритмов оптимизации. \n",
    "\n",
    "Функции активации (ReLU, Sigmoid, Tanh и другие) находятся в модуле torch.nn или torch.nn.functional. \n",
    "Например, nn.ReLU() применяется к выходу слоя для введения нелинейности. \n",
    "\n",
    "Оптимизаторы — в модуле torch.optim. Они инкапсулируют алгоритмы обновления параметров (SGD, Adam, RMSProp и т. д.). Чтобы использовать оптимизатор, нужно передать ему параметры модели и скорость обучения:\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)     # стохастический градиентный спуск\n",
    "# или\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)   # оптимизатор Адам \n",
    "```\n",
    "\n",
    "Во время обучения на каждой итерации обычно делают так:\n",
    "\n",
    "1. `optimizer.zero_grad()` — обнуляем накопленные градиенты у параметров.\n",
    "2. Вычисляем прямой проход и функцию потерь.\n",
    "3. `loss.backward()` — считаем градиенты через autograd.\n",
    "4. `optimizer.step()` — обновляем параметры по посчитанным градиентам.\n",
    "\n",
    "Это обеспечивает корректное обновление весов сети. \n",
    "\n",
    "Широкий выбор активаций и оптимизаторов позволяет настраивать архитектуру и обучение для разных задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f39b4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Весовой тензор fc1.weight до обучения (кусочек):\n",
      "tensor([[-0.1701, -0.1830,  0.2263],\n",
      "        [ 0.1084,  0.0169, -0.2946]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Весовой тензор fc1.weight после обучения (кусочек):\n",
      "tensor([[-0.1726, -0.1823,  0.2288],\n",
      "        [ 0.1081,  0.0170, -0.2950]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Значение лосса: 0.1942\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from SimpleNN import SimpleNN\n",
    "\n",
    "# 2. Создаём модель и оптимизатор\n",
    "model = SimpleNN(in_features=5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# 3. Генерируем случайные входные данные и метки\n",
    "x = torch.randn(3, 5)\n",
    "y = torch.randn(3, 1)\n",
    "\n",
    "\n",
    "# 4. Сохраняем веса первого слоя до обучения\n",
    "weights_before = model.fc1.weight.clone()\n",
    "\n",
    "\n",
    "# 5. Прямой проход и лосс\n",
    "outputs = model(x)\n",
    "loss = criterion(outputs, y)\n",
    "\n",
    "\n",
    "# 6. Шаг обучения\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "# 7. Сохраняем веса первого слоя после обучения\n",
    "weights_after = model.fc1.weight.clone()\n",
    "\n",
    "\n",
    "# 8. Выводим кусочек весов до и после, а также значение loss\n",
    "\n",
    "print(\"Весовой тензор fc1.weight до обучения (кусочек):\")\n",
    "print(weights_before[:2, :3])\n",
    "print(\"\\nВесовой тензор fc1.weight после обучения (кусочек):\")\n",
    "print(weights_after[:2, :3])\n",
    "print(f\"\\nЗначение лосса: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa02f7",
   "metadata": {},
   "source": [
    "Как можно видеть, тензоры весов после обучения действительно изменились"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aa65a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Вы рассмотрели ключевые компоненты PyTorch: \n",
    "\n",
    "- тензоры для хранения данных,\n",
    "- Autograd для автоматического подсчёта градиентов,\n",
    "- класс nn.Module для построения нейросетей,\n",
    "- а также Dataset и DataLoader для организации данных.\n",
    "\n",
    "Вы узнали, как применять функции активации и оптимизаторы для обучения моделей.\n",
    "\n",
    "Благодаря этому PyTorch предоставляет разработчикам гибкий и интуитивно понятный инструментарий для исследований и быстрого прототипирования нейросетей.\n",
    "\n",
    "Следующий шаг — сделать проекты более прозрачными и воспроизводимыми, даже если они растут до десятков разных запусков и конфигураций. В следующем уроке вы перейдёте от локального эксперимента к систематическому трекингу и анализу результатов: познакомитесь с фреймворком ClearML и научитесь интегрировать его в проект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24689c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-experiments-m4qsJGWw-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
