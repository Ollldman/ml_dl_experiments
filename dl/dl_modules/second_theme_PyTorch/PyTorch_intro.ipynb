{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59c47fd",
   "metadata": {},
   "source": [
    "# Что такое PyTorch\n",
    "\n",
    "PyTorch — это фреймворк машинного обучения для языка Python с открытым исходным кодом, созданный на базе библиотеки Torch.\n",
    "\n",
    "Он позволяет очень быстро и гибко описывать нейронные сети: вместо множества строк матричных умножений и ручных вычислений градиентов мы будем работать с понятными и интуитивными абстракциями.\n",
    "\n",
    "С его помощью можно с минимальными усилиями превратить ранее написанный вручную код обучения в лаконичные и надёжные конструкции: от создания собственных датасетов до настройки функций активации и выбор оптимизатора. Это поможет вам быстрее прототипировать идеи, сосредоточиться на дизайне моделей и легко масштабировать их в дальнейшем.\n",
    "\n",
    "PyTorch быстро стал популярным благодаря удобству разработки и гибкости. В отличие от фреймворков со статическим графом (например, TensorFlow), PyTorch использует динамический граф вычислений: граф создаётся прямо во время выполнения кода. Это делает его более гибким и упрощает отладку: вы можете изменять архитектуру сети во время работы и пользоваться привычными средствами Python (print, отладчик).\n",
    "\n",
    "Код на PyTorch выглядит естественно для Python-разработчика и очень похож на NumPy-нотации. Кроме того, PyTorch оптимизирован под GPU: он эффективно использует видеокарты для обучения крупных моделей. Фреймворк обладает большой стандартной библиотекой слоёв, функций потерь и оптимизаторов, а также обширным сообществом исследователей и разработчиков, которые создают новые модели и обучающие материалы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a7d23",
   "metadata": {},
   "source": [
    "# torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a28b1",
   "metadata": {},
   "source": [
    "В основе PyTorch лежит объект тензор — многомерный массив чисел, аналогичный numpy.ndarray, но с дополнительными возможностями.\n",
    "\n",
    "Тензоры могут храниться на CPU или GPU и, при необходимости, отслеживать вычисленные градиенты.\n",
    "\n",
    "Интерфейс PyTorch специально сделан похожим на NumPy, чтобы разработчику было удобно работать с данными.\n",
    "\n",
    "Давайте посмотрим на несколько тензоров в действии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4dcf38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.]) torch.float32\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]]) torch.int64\n",
      "tensor([[0.7798, 0.4136, 0.0503],\n",
      "        [0.2354, 0.7947, 0.9279],\n",
      "        [0.0162, 0.9562, 0.2846]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "a = torch.tensor([1.0, 2.0, 3.0])              # 1D-тензор из списка\n",
    "b = torch.zeros((2,3), dtype=torch.int64)      # матрица 2х3 из нулей(64-битные целы)\n",
    "c = torch.rand(3, 3)                           # случайный тензор 3х3, равномерное распределение\n",
    "\n",
    "print(a, a.dtype)\n",
    "print(b, b.dtype)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9fa37",
   "metadata": {},
   "source": [
    "Тензоры поддерживают стандартные арифметические операции (сложение, умножение, матричные произведения и другие) и функциональные преобразования. Запустите код, чтобы проверить, как работают арифметические операции в PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80491943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.7183, -1.8647, 23.0855])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.tensor([1.0, -2.0, 3.0])\n",
    "y = torch.exp(x)    # элемент-wise экспонента\n",
    "z = x + y           # сложение поэлементно\n",
    "print(z) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0be57",
   "metadata": {},
   "source": [
    "При необходимости тензоры можно передавать на GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4fcf6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    a = a.to('cuda')  # переместить на CUDA-устройство "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4b370",
   "metadata": {},
   "source": [
    "Поскольку PyTorch тесно интегрирован с NumPy, тензоры можно конвертировать в numpy.ndarray и обратно — например, через torch.from_numpy или метод tensor.numpy()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f82d13",
   "metadata": {},
   "source": [
    "# Автоматическое дифференцирование (Autograd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39ecc3",
   "metadata": {},
   "source": [
    "PyTorch имеет встроенную систему автоматического дифференцирования (autograd), которая вычисляет градиенты. Если у тензора установлено свойство requires_grad=True, PyTorch запоминает все операции над ним и может потом вычислить градиенты по цепному правилу. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf192e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)  # создаём тензор, указывая, что нужны градиенты\n",
    "y = x * 2\n",
    "\n",
    "z = y.mean()      # свёртка: усредняем все элементы\n",
    "z.backward()      # выполняем обратный проход (backpropagation)\n",
    "print(x.grad)     # смотрим градиент dz/dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde289e6",
   "metadata": {},
   "source": [
    "Вот так просто мы посчитали градиент нашего тензора!\n",
    "\n",
    "В этом примере z = (x*2).mean() = 2.0, и после z.backward() в x.grad окажутся градиенты dz/dx. Автоматическая дифференциация позволяет легко реализовать обратный проход при обучении нейросетей. Градиенты накапливаются в атрибуте .grad тензора и могут быть использованы оптимизатором для обновления параметров.\n",
    "\n",
    "Вы увидели, как автоматическое дифференцирование тщательно отслеживает операции с тензорами и накапливает градиенты. \n",
    "\n",
    "Следующий шаг — организовать сами вычисления в удобные блоки. Вместо того чтобы вручную писать последовательность матричных умножений и вызов backward(), мы хотим упаковать слои сети в единый объект с встроенным хранением параметров и логикой прямого прохода. Для этого и служит класс nn.Module, который упрощает создание, хранение и обучение любой архитектуры нейронной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bba255d",
   "metadata": {},
   "source": [
    "# Модели и nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d774e",
   "metadata": {},
   "source": [
    "В PyTorch нейронные сети строятся из модулей, основанных на базовом классе torch.nn.Module. Модуль nn.Module — это любое преобразование данных с параметрами (например, слой сети). Все стандартные слои (линейный слой, свёрточный слой, активации и т. д.) — это подклассы nn.Module. Саму нейросеть принято реализовывать как подкласс nn.Module, где в методе __init__ задаются слои, а в forward описывается логика прямого прохода.\n",
    "\n",
    "Подкласс nn.Module позволяет определить сразу две ключевые части любой нейросети:\n",
    "-  Инициализацию, где мы создаём слои и другие компоненты.\n",
    "-  Логику прямого прохода.\n",
    "\n",
    "Рассмотрим каждый метод по отдельности.\n",
    "Метод __init__ отвечает за инициализацию параметров и создание слоёв/функций активации. Когда мы наследуемся от nn.Module, первое, что нужно сделать внутри __init__, — это вызвать конструктор базового класса, чтобы внутри него успели инициализироваться внутренние механизмы PyTorch: регистрация параметров, хранение списка дочерних модулей и т. п."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "157b1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Важно: сначала вызываем конструктор nn.Module\n",
    "        # Создаём линейный слой: принимает 10 входов, выдаёт 5 выходов\n",
    "        self.fc1 = nn.Linear(in_features=10, out_features=5) \n",
    "        self.relu = nn.ReLU()         # ReLU-активация "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc7fed",
   "metadata": {},
   "source": [
    "super().__init__() запускает конструктор класса nn.Module. В результате внутри экземпляра появится структура, позволяющая хранить все слои, параметры (веса, смещения) и вложенные модули.\n",
    "\n",
    "Без вызова super().__init__() PyTorch не увидит созданные в дальнейшем слои и не сможет корректно собрать список параметров для оптимизатора.\n",
    "\n",
    "После инициализации базового класса можно добавить в наш класс любой набор слоёв и функций активации. Все элементы, присвоенные атрибутам экземпляра, которые сами являются подклассами nn.Module (например, nn.ReLU и т. д.), автоматически будут зарегистрированы как дочерние модули.\n",
    "\n",
    "nn.Linear(in_features, out_features) создаёт слой, который хранит матрицу весов размера (in_features, out_features) и вектор смещений длины out_features. \n",
    "\n",
    "При создании по умолчанию веса и смещения инициализируются случайно — например, из нормального распределения, сконструированного отдельными правилами.\n",
    "\n",
    "Параметры weights и bias, с которыми мы сейчас познакомимся, автоматически получают атрибут requires_grad=True, то есть по ним будут рассчитываться градиенты при расчёте обратного распространения ошибки — loss.backward().\n",
    "\n",
    "nn.ReLU() — это слой, который не содержит обучаемых параметров, а лишь преобразует входной тензор по правилу f(z) = max(0, z). Поскольку nn.ReLU() тоже наследуется от nn.Module, PyTorch знает, что этот модуль — часть графа, и сможет правильно вставить операцию ReLU в вычислительный граф."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85facd",
   "metadata": {},
   "source": [
    "Когда в __init__ мы объявили все слои и функции активации, метод forward отвечает за логику прямого прохода: он получает на вход батч данных, прокатывает его через все слои, применяя необходимые преобразования, и возвращает выход. Стандартная сигнатура выглядит так:\n",
    "\n",
    "```python\n",
    "\n",
    "def forward(self, x):\n",
    "    # x - входной тензор формы [batch_size, in_features]\n",
    "    # Здесь мы пишем последовательность операций вида:\n",
    "    # x = self.fc1(x)\n",
    "    # x = self.relu(x)\n",
    "    # x = self.fc2(x)\n",
    "    # return x\n",
    "\n",
    "```\n",
    "\n",
    "Параметр x  должен быть тензором с той размерностью, которую ожидает первый слой. Метод forward неявно вызывается при том, что мы делаем model(input_tensor). Под капотом PyTorch перенаправляет этот вызов в forward.\n",
    "\n",
    "<div background-color=green>\n",
    "Абсолютно любой код внутри forward будет использован при построении динамического графа. Можно писать условные операторы, циклы, вызывать другие функции и т. д. Всё, что приводит к операции над тензорами с requires_grad=True, станет частью графа, по которому потом будет проходить backward().\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be88e3",
   "metadata": {},
   "source": [
    "Как работает nn.Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0330ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Весовой тензор W: torch.Size([5, 10])\n",
      "Вектор смещений b: torch.Size([5])\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "model = nn.Linear(in_features=10, out_features=5)\n",
    "print(\"Весовой тензор W:\", model.weight.shape)   # torch.Size([5, 10])\n",
    "print(\"Вектор смещений b:\", model.bias.shape)    # torch.Size([5])\n",
    "\n",
    "\n",
    "# Проверим, что по умолчанию требуется вычисление градиентов\n",
    "print(model.weight.requires_grad)\n",
    "print(model.bias.requires_grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8f85d",
   "metadata": {},
   "source": [
    "Проверим работу RelU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9011610d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 2.],\n",
      "        [3., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([[-1.0, 0.0, 2.0], [3.0, -5.0, 1.0]])\n",
    "y = relu(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db7d4b",
   "metadata": {},
   "source": [
    "Пошаговый пример\n",
    "Допустим, есть два линейных слоя и одна активация ReLU. Логика такая:\n",
    "\n",
    "1. На вход в forward приходит тензор x формы [batch_size, 10].\n",
    "2. Сначала мы применяем линейное преобразование — есть матрица весов (10 → 5), получается тензор формы [batch_size, 5].\n",
    "3. К полученному результату применяем ReLU.\n",
    "4. Результат ReLU снова пропускаем через линейный слой (5 → 1), получаем [batch_size, 1].\n",
    "5. Метод возвращает этот выходной тензор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976762a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7281,  0.9982,  0.7531,  0.8091,  2.0482, -0.1595, -0.8416, -0.4780,\n",
      "          1.0232, -0.0787],\n",
      "        [-0.9667,  0.2407, -0.3744,  0.7973,  1.7687, -0.8267,  0.1825,  0.2015,\n",
      "          1.5891, -0.1798],\n",
      "        [-0.0836, -0.7374, -1.4650,  1.0841,  0.0646, -0.6541,  0.8034, -0.9726,\n",
      "          0.1947, -1.0811]])\n",
      "tensor([[0.3407],\n",
      "        [0.4364],\n",
      "        [0.5301]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# Предположим, что мы уже создали в __init__ следующие слои:\n",
    "fc1 = nn.Linear(10, 5)     # из 10 входов в 5 нейронов\n",
    "relu = nn.ReLU()\n",
    "fc2 = nn.Linear(5, 1)      # из 5 входов в 1 нейрон\n",
    "\n",
    "\n",
    "# А вот упрощённая функция forward, написанная вне класса:\n",
    "def forward_pass(x):\n",
    "    x1 = fc1(x)            # применили первый линейный слой\n",
    "    x2 = relu(x1)          # применили ReLU\n",
    "    x3 = fc2(x2)           # применили второй линейный слой\n",
    "    return x3\n",
    "\n",
    "\n",
    "# Создадим тестовый входной батч: batch_size=3, in_features=10\n",
    "\n",
    "input_tensor = torch.randn(3, 10)\n",
    "output_tensor = forward_pass(input_tensor)\n",
    "print(input_tensor)\n",
    "print(output_tensor)  # tensor of shape [3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c8ecfc",
   "metadata": {},
   "source": [
    "Видно, что мы неявно строим динамический граф: когда выполняются `fc1(x)`, `relu(x1)` и `fc2(x2)`, PyTorch запоминает, какие операции были применены и к каким входным тензорам. \n",
    "\n",
    "В дальнейшем, если мы сформируем функцию потерь от `output_tensor` и вызовем `loss.backward()`, будут вычислены градиенты по всем параметрам `fc1.weight`, `fc1.bias`, `fc2.weight`, `fc2.bias`, вы можете самостоятельно добавить в код-сниппет выводы этих параметров и посмотреть, как они будут выглядеть.\n",
    "\n",
    "**Что должен ожидать метод forward на вход**:\n",
    "\n",
    "- Обычно это тензор формы (batch_size, in_features) — если вы работаете с табличными данными или одним линейным слоем.\n",
    "\n",
    "- В случае с изображениями это может быть тензор (batch_size, channels, height, width).\n",
    "\n",
    "- Главное, чтобы форма тензора совпадала с формой, на которую настроен первый слой. Например, если первый слой — nn.Linear(10, 5), то входные данные должны иметь вторую размерность 10.\n",
    "\n",
    "**Что должен отдавать метод forward на выход**:\n",
    "\n",
    "- Как правило, это тензор (batch_size, out_features), где out_features — число выходных нейронов в последнем слое.\n",
    "- Например, при регрессии на одно число мы получим (batch_size, 1), при классификации на 10 классов — (batch_size, 10) и т. д.\n",
    "- Внутри forward мы можем применять дополнительные нелинейности, нормализации, дропауты, но результатом всегда должен быть тензор, с которым затем мы сравним выход с метками и посчитаем лосс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69717c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n",
      "Всего параметров: 61\n"
     ]
    }
   ],
   "source": [
    "from SimpleNN import SimpleNN\n",
    "# Пример использования:\n",
    "model = SimpleNN()\n",
    "print(model)\n",
    "\n",
    "\n",
    "# Можно сразу посмотреть, сколько параметров зарегистрировано:\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Всего параметров: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebcd261",
   "metadata": {},
   "source": [
    "При вызове model(input_tensor) внутри автоматически сработает forward, и на выходе появятся предсказания.\n",
    "\n",
    "Все параметры (fc1.weight, fc1.bias, fc2.weight, fc2.bias) автоматически попадут в model.parameters(), поэтому мы сможем передать их в оптимизатор, например:\n",
    "\n",
    "```python\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8504f8e4",
   "metadata": {},
   "source": [
    "Если впоследствии мы вычислим лосс на основе предсказаний и метода loss.backward(), PyTorch аккуратно посчитает градиенты по всем весам и смещениям, а затем optimizer.step() обновит их.\n",
    "Таким образом, мы подробно разобрали, как внутри nn.Module организованы ключевые методы:\n",
    "\n",
    "    __init__ — для создания и регистрации слоёв,\n",
    "    forward — для описания потока данных.\n",
    "\n",
    "Вместе они образуют простую, но полную структуру нейросети, которую можно сразу обучать или расширять дополнительными слоями и функциональностью:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4905342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dl-experiments-m4qsJGWw-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
