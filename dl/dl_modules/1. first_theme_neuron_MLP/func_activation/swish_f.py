import numpy as np
from numpy.typing import ArrayLike

from func_activation import sigmoid


def swish(x: ArrayLike, beta: float = 1.) -> np.ndarray:
    """
    Функцию активации Swish предложили исследователи из Google. Swish улучшает производительность модели по сравнению с традиционными функциями активации, такими как ReLU. Но точность растёт мало, а вычислительных затрат много, поэтому на практике функция не популярна. 

    Плюсы:

    Плавность и дифференцируемость на всём диапазоне.
    Лёгкая «утечка» положительных и отрицательных частей — меньше «мёртвых» нейронов.
    Демонстрирует улучшение качества на некоторых бенчмарках.

    Минусы:

    Вычислительно дороже ReLU — нужно вычислить экспоненту и умножение.
    Более сложная форма может замедлять обучение по сравнению с простыми функциями.
    """
    x = np.asarray(x)
    return x * sigmoid(beta * x)
