import numpy as np
from numpy.typing import ArrayLike, NDArray

def tanh(x: ArrayLike) -> NDArray[np.float64]:
    """
    tanh(x) = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

    Эта функция активации может использоваться в скрытых слоях нейросетей. 
    
    Плюсы:

    Выходы центрированы вокруг нуля — ускоряет сходимость, то есть быстрее обучается.
    В центральной области градиенты более крутые, чем у сигмоиды. Это смягчает проблему затухающих градиентов.

    Минусы:

    Остаётся проблема затухающих градиентов. При больших по модулю значениях x градиент затухает, из-за этого глубокие сети медленно обучаются.
    
    Вычислительно затратнее сигмоиды.
    """
    x = np.asarray(x, dtype=np.float64)
    return np.tanh(x)