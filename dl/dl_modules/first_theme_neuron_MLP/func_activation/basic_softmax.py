import numpy as np
from numpy.typing import ArrayLike


def softmax(z: ArrayLike) -> ArrayLike:
    """
    По работе эта функция похожа на Sigmoid(x). Если нужно получить одну вероятность из одного числа, можно вызвать Sigmoid(x). Если классов два, то обе функции дают одну и ту же S-образную кривую. Отличие Softmax в том, что она справляется и с большим количеством классов — K>2.

    Плюсы:

    Выдаёт нормированное распределение вероятностей.

    Подходит для последнего слоя многоклассовых задач.

    Минусы:

    Вычислительная сложность. Для каждого элемента нужно вычислять экспоненту.

    Возможна числовая нестабильность. При больших по модулю значениях логитов прямая экспонента может привести к переполнению или потере точности. Поэтому перед расчётами нужно делать стабилизацию.
    
    """
    # Для стабильности вычитаем максимум
    e_z = np.exp(z - np.max(z))

    return e_z / e_z.sum()